{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-08 01:57:50.074678: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils import f1_score_function\n",
    "from BertClassifier import BertClassifier, init_bert_clf, train_bert_clf, eval_bert_clf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_csv('subtaskA_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>âš¡Se non ci fossero soldati non ci sarebbero gu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21/08/21]( [PRE-PRINT]\\n\\nðŸ“„__ \"Shedding of Inf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L'Aspirina non aumenta la sopravvivenza dei pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L'Italia non puo' dare armi lo vieta la Costit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  conspiratorial\n",
       "0  âš¡Se non ci fossero soldati non ci sarebbero gu...               0\n",
       "1  21/08/21]( [PRE-PRINT]\\n\\nðŸ“„__ \"Shedding of Inf...               1\n",
       "2  PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...               1\n",
       "3  L'Aspirina non aumenta la sopravvivenza dei pa...               0\n",
       "4  L'Italia non puo' dare armi lo vieta la Costit...               0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1845 entries, 0 to 1844\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    1842 non-null   object\n",
      " 1   conspiratorial  1845 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 29.0+ KB\n"
     ]
    }
   ],
   "source": [
    "texts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    comment_text  conspiratorial\n",
       "244          NaN               0\n",
       "263          NaN               0\n",
       "665          NaN               0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df[texts_df['comment_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = texts_df[texts_df.comment_text.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1842 entries, 0 to 1844\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    1842 non-null   object\n",
      " 1   conspiratorial  1842 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.2+ KB\n"
     ]
    }
   ],
   "source": [
    "texts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    925\n",
       "0    917\n",
       "Name: conspiratorial, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.conspiratorial.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove break line characthers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.comment_text = texts_df.comment_text.apply(lambda text: text.replace('\\n\\n', ' ').replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80-20 train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing datasets using stratified sampling\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, eval_index in split.split(texts_df, texts_df.conspiratorial):\n",
    "    train_df, val_df = texts_df.iloc[train_index], texts_df.iloc[eval_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1473 entries, 1512 to 1771\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    1473 non-null   object\n",
      " 1   conspiratorial  1473 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 34.5+ KB\n",
      "None\n",
      "1    740\n",
      "0    733\n",
      "Name: conspiratorial, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())\n",
    "print(train_df.conspiratorial.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 369 entries, 363 to 670\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    369 non-null    object\n",
      " 1   conspiratorial  369 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.6+ KB\n",
      "None\n",
      "1    185\n",
      "0    184\n",
      "Name: conspiratorial, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(val_df.info())\n",
    "print(val_df.conspiratorial.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-italian-xxl-cased', do_lower_case=False)\n",
    "\n",
    "texts_tr = train_df['comment_text']\n",
    "labels_tr = train_df['conspiratorial'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tr = train_df['comment_text']\n",
    "labels_tr = train_df['conspiratorial'].to_numpy()\n",
    "\n",
    "texts_val = val_df['comment_text']\n",
    "labels_val = val_df['conspiratorial'].to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe distribution of tokenized texts lengths by trying a simple tokenization on both tr and val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_len_list = []\n",
    "\n",
    "for sentence in texts_tr:\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    tokenized_len_list.append(len(input_ids))\n",
    "for sentence in texts_val:\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    tokenized_len_list.append(len(input_ids))\n",
    "\n",
    "sns.displot(tokenized_len_list)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum tokenized length is above the BERT max_lenght limit of 512. Very few texts are above this limit, so we truncate to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "\n",
    "# Tr set\n",
    "input_ids_tr = []\n",
    "attention_masks_tr = []\n",
    "\n",
    "# `encode_plus` will:\n",
    "#   1. Tokenize the sentence, 2. Prepend the `[CLS]` token to the start, 3. Append the `[SEP]` token to the end\n",
    "#   4. Map tokens to their IDs, 5. Pad or truncate the sentence to `max_length`, 6. Create attention masks for [PAD] tokens\n",
    "\n",
    "for sentence in texts_tr:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence and attention mask to the list.    \n",
    "    input_ids_tr.append(encoded_dict['input_ids'])\n",
    "    attention_masks_tr.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Val set\n",
    "input_ids_val = []\n",
    "attention_masks_val = []\n",
    "\n",
    "for sentence in texts_val:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence and attention mask to the list.    \n",
    "    input_ids_val.append(encoded_dict['input_ids'])\n",
    "    attention_masks_val.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists into tensors\n",
    "\n",
    "input_ids_tr = torch.cat(input_ids_tr, dim=0)\n",
    "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
    "\n",
    "attention_masks_tr = torch.cat(attention_masks_tr, dim=0)\n",
    "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
    "\n",
    "labels_tr = torch.tensor(labels_tr)\n",
    "labels_val = torch.tensor(labels_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap data into a TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = TensorDataset(input_ids_tr, attention_masks_tr, labels_tr)\n",
    "val_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLoader needs to know our batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DataLoaders for our training and validation sets. Tr samples are taken in random order, while validation are taken sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataloader = DataLoader(tr_dataset, sampler=RandomSampler(tr_dataset), batch_size = batch_size)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size = batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set save folder for this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-08 01:57:53.844599\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "curr_date = datetime.now()\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "save_folder_pth = './bert_subtaskA/{}_{}_{}-{}.{}'.format(curr_date.day, curr_date.month, curr_date.day, curr_date.hour, curr_date.minute)\n",
    "if not os.path.exists(save_folder_pth):\n",
    "    os.makedirs(save_folder_pth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed general hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "\n",
    "\n",
    "# Num batches*num epochs\n",
    "tr_steps = len(tr_dataloader)*max_epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable grid searched hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.5e-5, 0.2e-5, 0.8e-5]\n",
    "warmup_perc_list = [0.1, 0.05] # Percentage of warmup steps for scheduler on the total tr steps\n",
    "clf_head_list = []\n",
    "\n",
    "head1 = nn.Sequential(\n",
    "                nn.Linear(768, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25),\n",
    "                nn.Linear(128, 2),\n",
    "                )\n",
    "clf_head_list.append((head1, '2_layers_S')) # Each head obeject is composed by a tuple (head, name)\n",
    "\n",
    "head2 = nn.Sequential(\n",
    "                nn.Linear(768, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25),\n",
    "                nn.Linear(256, 2),\n",
    "                )\n",
    "clf_head_list.append((head2, '2_layers_M'))\n",
    "\n",
    "head3 = nn.Sequential(\n",
    "                nn.Linear(768, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 2),\n",
    "                )\n",
    "clf_head_list.append((head3, '3_layers_M'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize, train/eval and save function for each grid search run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(lr, warmup_steps, head, model_folder_pth):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    #print(torch.cuda.memory_summary())\n",
    "\n",
    "    # Initialize model\n",
    "    bert_clf_model, loss_function, optimizer, scheduler, device = init_bert_clf(tr_steps=tr_steps, lr_rate=lr, scheduler_warmp_steps=warmup_steps, head=head)\n",
    "\n",
    "    for epoch_i in range(max_epochs):\n",
    "        print('Epoch: {}'.format(epoch_i))\n",
    "\n",
    "        # Train\n",
    "        avg_epoch_loss_tr, acc_score_tr, f1_score_tr, bert_clf_model, optimizer, scheduler = train_bert_clf(bert_clf_model, tr_dataloader, loss_function, optimizer, scheduler, device)\n",
    "\n",
    "\n",
    "        print('TRAINING | Tr Loss: {} - Tr acc: {} - Tr f1: {}'.format(avg_epoch_loss_tr, acc_score_tr, f1_score_tr))\n",
    "\n",
    "        # Eval\n",
    "        avg_epoch_loss_val, acc_score_val, f1_score_val, predictions, labels = eval_bert_clf(bert_clf_model, val_dataloader, loss_function, device)\n",
    "        print('EVALUATION | Val Loss: {} - Val acc: {} - Val f1: {}'.format(avg_epoch_loss_val, acc_score_val, f1_score_val))\n",
    "\n",
    "        # Save\n",
    "        model_save_pth = os.path.join(model_folder_pth, 'bert_clf.pt')\n",
    "        torch.save({\n",
    "                    'epoch': epoch_i,\n",
    "                    'model_state_dict': bert_clf_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'schdeduler_state_dict': scheduler.state_dict(),\n",
    "                    'tr_loss': avg_epoch_loss_tr,\n",
    "                    'val_loss': avg_epoch_loss_val,\n",
    "                    'tr_acc': acc_score_tr,\n",
    "                    'val_acc': acc_score_val,\n",
    "                    'tr_f1': f1_score_tr,\n",
    "                    'val_f1': f1_score_val,\n",
    "                    'val_preds': predictions\n",
    "                    }, model_save_pth)\n",
    "\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 18 trials in grid search\n",
      "Executing model with lr=5e-06, warmup perc.=0.1, head=2_layers_S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6616752782175618 - Tr acc: 0.6021724372029871 - Tr f1: 0.5999004360788502\n",
      "EVALUATION | Val Loss: 0.5829866540928682 - Val acc: 0.7262872628726287 - Val f1: 0.7249215780344689\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.5115685986895715 - Tr acc: 0.7549219280380176 - Tr f1: 0.7541968882218099\n",
      "EVALUATION | Val Loss: 0.46587145204345387 - Val acc: 0.7994579945799458 - Val f1: 0.798675828712988\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.37058374205584166 - Tr acc: 0.8397827562797013 - Tr f1: 0.8394096516449483\n",
      "EVALUATION | Val Loss: 0.4368150470157464 - Val acc: 0.7940379403794038 - Val f1: 0.7940243257535696\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.24544560104127852 - Tr acc: 0.9110658520027155 - Tr f1: 0.9110066250515961\n",
      "EVALUATION | Val Loss: 0.5261425226926804 - Val acc: 0.7994579945799458 - Val f1: 0.7982116464676323\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.1570727543164325 - Tr acc: 0.955193482688391 - Tr f1: 0.9551924707791011\n",
      "EVALUATION | Val Loss: 0.6279588658362627 - Val acc: 0.7777777777777778 - Val f1: 0.775\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.12136494545566459 - Tr acc: 0.9633401221995926 - Tr f1: 0.963338753567399\n",
      "EVALUATION | Val Loss: 0.6787408577899138 - Val acc: 0.7859078590785907 - Val f1: 0.7842855768590393\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.07729891320109687 - Tr acc: 0.9809911744738629 - Tr f1: 0.9809896937627907\n",
      "EVALUATION | Val Loss: 0.7847956319650015 - Val acc: 0.7886178861788617 - Val f1: 0.7880559646539027\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.06101752410051964 - Tr acc: 0.9823489477257298 - Tr f1: 0.9823475727797342\n",
      "EVALUATION | Val Loss: 0.840423759073019 - Val acc: 0.7831978319783198 - Val f1: 0.7820308346624136\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.052012840446124795 - Tr acc: 0.9898167006109979 - Tr f1: 0.9898162312542835\n",
      "EVALUATION | Val Loss: 0.8669059053063393 - Val acc: 0.7913279132791328 - Val f1: 0.7904414205111185\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.06140600196436368 - Tr acc: 0.9850644942294636 - Tr f1: 0.9850636612696856\n",
      "EVALUATION | Val Loss: 0.872762031853199 - Val acc: 0.7886178861788617 - Val f1: 0.7874800637958533\n",
      "Executing model with lr=5e-06, warmup perc.=0.05, head=2_layers_S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6017272328176806 - Tr acc: 0.6639511201629328 - Tr f1: 0.6618540416728309\n",
      "EVALUATION | Val Loss: 0.5148275730510553 - Val acc: 0.7425474254742548 - Val f1: 0.7420624701077958\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.42955559964782447 - Tr acc: 0.7976917854718262 - Tr f1: 0.7975192353820322\n",
      "EVALUATION | Val Loss: 0.4474239870905876 - Val acc: 0.7831978319783198 - Val f1: 0.7830049985298442\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.2898616098187944 - Tr acc: 0.8825526137135098 - Tr f1: 0.8825309577293255\n",
      "EVALUATION | Val Loss: 0.45569646389534074 - Val acc: 0.8211382113821138 - Val f1: 0.8208421610169492\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.20136663739279073 - Tr acc: 0.9266802443991853 - Tr f1: 0.9266653390421189\n",
      "EVALUATION | Val Loss: 0.5214527218292156 - Val acc: 0.7967479674796748 - Val f1: 0.7965985844584414\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.15004650847886197 - Tr acc: 0.9606245756958588 - Tr f1: 0.9606223797109893\n",
      "EVALUATION | Val Loss: 0.5941877256458005 - Val acc: 0.8048780487804879 - Val f1: 0.8046355667980469\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.07520402445187492 - Tr acc: 0.9789545145960624 - Tr f1: 0.978953544592186\n",
      "EVALUATION | Val Loss: 0.6833205794294676 - Val acc: 0.8238482384823849 - Val f1: 0.8238275368173639\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.05711237222497021 - Tr acc: 0.988458927359131 - Tr f1: 0.9884588422524085\n",
      "EVALUATION | Val Loss: 0.7194584948010743 - Val acc: 0.8075880758807588 - Val f1: 0.8075371900826446\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.05291489670453693 - Tr acc: 0.988458927359131 - Tr f1: 0.9884578847152821\n",
      "EVALUATION | Val Loss: 0.7501317257216821 - Val acc: 0.8130081300813008 - Val f1: 0.8126558993414518\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.04815097543240715 - Tr acc: 0.988458927359131 - Tr f1: 0.9884585869247096\n",
      "EVALUATION | Val Loss: 0.7611627786730727 - Val acc: 0.8102981029810298 - Val f1: 0.8099841101694916\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.04211171904218293 - Tr acc: 0.9904955872369314 - Tr f1: 0.9904952324063627\n",
      "EVALUATION | Val Loss: 0.7659142012707889 - Val acc: 0.8130081300813008 - Val f1: 0.8126558993414518\n",
      "Executing model with lr=5e-06, warmup perc.=0.1, head=2_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6488303891433183 - Tr acc: 0.6272912423625254 - Tr f1: 0.6143333576557745\n",
      "EVALUATION | Val Loss: 0.5751154298583666 - Val acc: 0.7046070460704607 - Val f1: 0.702896356101849\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.5482894451387467 - Tr acc: 0.736591989137814 - Tr f1: 0.7363235402672177\n",
      "EVALUATION | Val Loss: 0.49888673176368076 - Val acc: 0.7669376693766937 - Val f1: 0.7666480381198895\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.435359797971223 - Tr acc: 0.7983706720977597 - Tr f1: 0.7981906021469762\n",
      "EVALUATION | Val Loss: 0.46970086358487606 - Val acc: 0.7777777777777778 - Val f1: 0.7776455026455027\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.3317755988688879 - Tr acc: 0.8587915818058385 - Tr f1: 0.8586085319254979\n",
      "EVALUATION | Val Loss: 0.5240543633699417 - Val acc: 0.7642276422764228 - Val f1: 0.7586694230407818\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.2581474572740575 - Tr acc: 0.8995247793618466 - Tr f1: 0.8994910096818811\n",
      "EVALUATION | Val Loss: 0.5450106697777907 - Val acc: 0.7831978319783198 - Val f1: 0.7816568047337278\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.18057805149545592 - Tr acc: 0.9341479972844535 - Tr f1: 0.9341241940205952\n",
      "EVALUATION | Val Loss: 0.6725268959999084 - Val acc: 0.7588075880758808 - Val f1: 0.7546560882135414\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.13354700657549085 - Tr acc: 0.9545145960624576 - Tr f1: 0.9545062090628025\n",
      "EVALUATION | Val Loss: 0.6684203116844097 - Val acc: 0.7723577235772358 - Val f1: 0.770291100966384\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.12484764289711753 - Tr acc: 0.9606245756958588 - Tr f1: 0.9606180233137025\n",
      "EVALUATION | Val Loss: 0.6758979291965564 - Val acc: 0.7750677506775068 - Val f1: 0.7742653507963767\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.09479387534121352 - Tr acc: 0.9694501018329938 - Tr f1: 0.969444468773524\n",
      "EVALUATION | Val Loss: 0.7112846613551179 - Val acc: 0.7669376693766937 - Val f1: 0.7654892107596807\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.08797155714203272 - Tr acc: 0.9708078750848609 - Tr f1: 0.9708013617641162\n",
      "EVALUATION | Val Loss: 0.7173840102429191 - Val acc: 0.7696476964769647 - Val f1: 0.7681149777097611\n",
      "Executing model with lr=5e-06, warmup perc.=0.05, head=2_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6051030953725179 - Tr acc: 0.6632722335369993 - Tr f1: 0.6590977808469737\n",
      "EVALUATION | Val Loss: 0.5523378998041153 - Val acc: 0.7289972899728997 - Val f1: 0.7213832678948958\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.46005498962376706 - Tr acc: 0.7841140529531568 - Tr f1: 0.7835529161260272\n",
      "EVALUATION | Val Loss: 0.44878147915005684 - Val acc: 0.7967479674796748 - Val f1: 0.7965985844584414\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.3464493165093084 - Tr acc: 0.845213849287169 - Tr f1: 0.8451052666533838\n",
      "EVALUATION | Val Loss: 0.4513953433682521 - Val acc: 0.7940379403794038 - Val f1: 0.7929292929292928\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.21390960910307463 - Tr acc: 0.923285811269518 - Tr f1: 0.923261902775082\n",
      "EVALUATION | Val Loss: 0.6190054041023055 - Val acc: 0.7669376693766937 - Val f1: 0.7630942427138079\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.14591799143661735 - Tr acc: 0.9490835030549898 - Tr f1: 0.9490826582378245\n",
      "EVALUATION | Val Loss: 0.5905195598800977 - Val acc: 0.7967479674796748 - Val f1: 0.7960229073461236\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.09698864133648014 - Tr acc: 0.9653767820773931 - Tr f1: 0.9653736541458462\n",
      "EVALUATION | Val Loss: 0.7155352442835768 - Val acc: 0.7696476964769647 - Val f1: 0.7656827808781013\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.06707819501158371 - Tr acc: 0.9823489477257298 - Tr f1: 0.9823479633187193\n",
      "EVALUATION | Val Loss: 0.7553442550512651 - Val acc: 0.7859078590785907 - Val f1: 0.7830978370052902\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.04951380069557858 - Tr acc: 0.9850644942294636 - Tr f1: 0.9850636612696856\n",
      "EVALUATION | Val Loss: 0.8299667794878284 - Val acc: 0.7831978319783198 - Val f1: 0.7799248523886205\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.04444327713140557 - Tr acc: 0.9877800407331976 - Tr f1: 0.9877795845224663\n",
      "EVALUATION | Val Loss: 0.8455701073010763 - Val acc: 0.7859078590785907 - Val f1: 0.7828198919724344\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.04982724527437841 - Tr acc: 0.9850644942294636 - Tr f1: 0.9850629452630648\n",
      "EVALUATION | Val Loss: 0.831163800942401 - Val acc: 0.7831978319783198 - Val f1: 0.7802132348561559\n",
      "Executing model with lr=5e-06, warmup perc.=0.1, head=3_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6808151320744587 - Tr acc: 0.5621181262729125 - Tr f1: 0.4868934610692292\n",
      "EVALUATION | Val Loss: 0.6451592122515043 - Val acc: 0.6531165311653117 - Val f1: 0.6440156768164004\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.5956822845243639 - Tr acc: 0.7114731839782756 - Tr f1: 0.7092778909092597\n",
      "EVALUATION | Val Loss: 0.5666727051138878 - Val acc: 0.7398373983739838 - Val f1: 0.7351674641148325\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.5216889525613477 - Tr acc: 0.790224032586558 - Tr f1: 0.7900192515914972\n",
      "EVALUATION | Val Loss: 0.5326383275290331 - Val acc: 0.7615176151761518 - Val f1: 0.7615018508725542\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.409321070839 - Tr acc: 0.8445349626612356 - Tr f1: 0.8445002754414555\n",
      "EVALUATION | Val Loss: 0.5087714232504368 - Val acc: 0.7831978319783198 - Val f1: 0.7823522472572846\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.338710028958577 - Tr acc: 0.8778004073319755 - Tr f1: 0.877795845224663\n",
      "EVALUATION | Val Loss: 0.5268085716913143 - Val acc: 0.7994579945799458 - Val f1: 0.7993386243386243\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.27075552507754297 - Tr acc: 0.9151391717583164 - Tr f1: 0.9151084974492207\n",
      "EVALUATION | Val Loss: 0.5658388634522756 - Val acc: 0.7940379403794038 - Val f1: 0.7930881832133161\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.23389701897739082 - Tr acc: 0.9293957909029192 - Tr f1: 0.9293902911189366\n",
      "EVALUATION | Val Loss: 0.585556834936142 - Val acc: 0.7669376693766937 - Val f1: 0.7648218414655836\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.20132586512193884 - Tr acc: 0.9477257298031229 - Tr f1: 0.9477222602479276\n",
      "EVALUATION | Val Loss: 0.6145630143582821 - Val acc: 0.7588075880758808 - Val f1: 0.7539207385150273\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.21307482298022956 - Tr acc: 0.9545145960624576 - Tr f1: 0.9545142606418452\n",
      "EVALUATION | Val Loss: 0.6195384822785854 - Val acc: 0.7642276422764228 - Val f1: 0.761133061005826\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.19153108800290733 - Tr acc: 0.9538357094365241 - Tr f1: 0.9538339859737617\n",
      "EVALUATION | Val Loss: 0.6194209220508734 - Val acc: 0.7669376693766937 - Val f1: 0.7640243902439023\n",
      "Executing model with lr=5e-06, warmup perc.=0.05, head=3_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6066545177531498 - Tr acc: 0.6802443991853361 - Tr f1: 0.6760301750404505\n",
      "EVALUATION | Val Loss: 0.5265903249382973 - Val acc: 0.7398373983739838 - Val f1: 0.7397437404490419\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.48219339277154655 - Tr acc: 0.7718940936863544 - Tr f1: 0.7717340771734078\n",
      "EVALUATION | Val Loss: 0.46440625935792923 - Val acc: 0.7913279132791328 - Val f1: 0.7895411281063665\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.3370569882694111 - Tr acc: 0.869653767820774 - Tr f1: 0.8696436143734214\n",
      "EVALUATION | Val Loss: 0.48843745825191337 - Val acc: 0.8048780487804879 - Val f1: 0.804876615746181\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.23864915762697497 - Tr acc: 0.9164969450101833 - Tr f1: 0.9164870915655554\n",
      "EVALUATION | Val Loss: 0.5457796386132637 - Val acc: 0.7913279132791328 - Val f1: 0.7910271025631597\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.1612239817297587 - Tr acc: 0.9497623896809233 - Tr f1: 0.9497571795212179\n",
      "EVALUATION | Val Loss: 0.6569406297057867 - Val acc: 0.8102981029810298 - Val f1: 0.8102855631940773\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.11649730286851365 - Tr acc: 0.9708078750848609 - Tr f1: 0.9708059375410505\n",
      "EVALUATION | Val Loss: 0.6836142707616091 - Val acc: 0.7777777777777778 - Val f1: 0.7759861407249466\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.09232417944698565 - Tr acc: 0.9789545145960624 - Tr f1: 0.9789541654049674\n",
      "EVALUATION | Val Loss: 0.8229249805832902 - Val acc: 0.7723577235772358 - Val f1: 0.7689210950080515\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.0805621666092706 - Tr acc: 0.9823489477257298 - Tr f1: 0.9823487443448506\n",
      "EVALUATION | Val Loss: 0.8486649844174584 - Val acc: 0.7777777777777778 - Val f1: 0.7755208024215087\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.06769584872890064 - Tr acc: 0.988458927359131 - Tr f1: 0.9884587358672402\n",
      "EVALUATION | Val Loss: 0.8606744005034367 - Val acc: 0.7723577235772358 - Val f1: 0.7705223880597015\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.0697568606845634 - Tr acc: 0.9843856076035302 - Tr f1: 0.9843854924591409\n",
      "EVALUATION | Val Loss: 0.8647125295052925 - Val acc: 0.7696476964769647 - Val f1: 0.7681149777097611\n",
      "Executing model with lr=2e-06, warmup perc.=0.1, head=2_layers_S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6408054081983464 - Tr acc: 0.6374745417515275 - Tr f1: 0.633624415909616\n",
      "EVALUATION | Val Loss: 0.5989057185749213 - Val acc: 0.6639566395663956 - Val f1: 0.6563213556062973\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.5388336210481582 - Tr acc: 0.7277664630006789 - Tr f1: 0.7274001032853319\n",
      "EVALUATION | Val Loss: 0.5069984166572491 - Val acc: 0.7615176151761518 - Val f1: 0.7614317620782884\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.4580212648837797 - Tr acc: 0.7983706720977597 - Tr f1: 0.7979893176729389\n",
      "EVALUATION | Val Loss: 0.47743623207012814 - Val acc: 0.7506775067750677 - Val f1: 0.7503676686863934\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.39168602380380835 - Tr acc: 0.8282416836388323 - Tr f1: 0.8281019296898243\n",
      "EVALUATION | Val Loss: 0.45547330503662425 - Val acc: 0.7804878048780488 - Val f1: 0.7795552605376701\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.3520564102036979 - Tr acc: 0.8499660556687033 - Tr f1: 0.8497641466983998\n",
      "EVALUATION | Val Loss: 0.4412979328384002 - Val acc: 0.7886178861788617 - Val f1: 0.7884298735665981\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.3314103293482975 - Tr acc: 0.8649015614392397 - Tr f1: 0.8648594570994808\n",
      "EVALUATION | Val Loss: 0.4412309266626835 - Val acc: 0.7967479674796748 - Val f1: 0.7965985844584414\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.2960153661908642 - Tr acc: 0.8757637474541752 - Tr f1: 0.87570508667615\n",
      "EVALUATION | Val Loss: 0.44393276671568555 - Val acc: 0.7994579945799458 - Val f1: 0.7992087769868815\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.27796912928342177 - Tr acc: 0.8934147997284454 - Tr f1: 0.8933815817317511\n",
      "EVALUATION | Val Loss: 0.4416584564993779 - Val acc: 0.8075880758807588 - Val f1: 0.8074466599539913\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.2716262117829374 - Tr acc: 0.8920570264765784 - Tr f1: 0.8919773685052873\n",
      "EVALUATION | Val Loss: 0.4408314935863018 - Val acc: 0.8048780487804879 - Val f1: 0.8047619047619048\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.27377419936801156 - Tr acc: 0.8934147997284454 - Tr f1: 0.8933815817317511\n",
      "EVALUATION | Val Loss: 0.44092989961306256 - Val acc: 0.8048780487804879 - Val f1: 0.8047619047619048\n",
      "Executing model with lr=2e-06, warmup perc.=0.05, head=2_layers_S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6082942405695556 - Tr acc: 0.6632722335369993 - Tr f1: 0.6620934620934622\n",
      "EVALUATION | Val Loss: 0.5611675195395947 - Val acc: 0.7452574525745257 - Val f1: 0.7434467455621301\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.48573745386574857 - Tr acc: 0.7549219280380176 - Tr f1: 0.7542327877262675\n",
      "EVALUATION | Val Loss: 0.47680577139059704 - Val acc: 0.7560975609756098 - Val f1: 0.7560527707586531\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.41228174995030126 - Tr acc: 0.8078750848608283 - Tr f1: 0.8077035030558729\n",
      "EVALUATION | Val Loss: 0.46235685236752033 - Val acc: 0.7777777777777778 - Val f1: 0.7776455026455027\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.3614129537856707 - Tr acc: 0.856754921928038 - Tr f1: 0.8566269934444848\n",
      "EVALUATION | Val Loss: 0.4581067804247141 - Val acc: 0.7994579945799458 - Val f1: 0.798675828712988\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.31140357800709306 - Tr acc: 0.8730482009504412 - Tr f1: 0.8729242719987378\n",
      "EVALUATION | Val Loss: 0.4431887815395991 - Val acc: 0.7967479674796748 - Val f1: 0.7960229073461236\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.29464143903184964 - Tr acc: 0.8859470468431772 - Tr f1: 0.8859192328969205\n",
      "EVALUATION | Val Loss: 0.4451106737057368 - Val acc: 0.8048780487804879 - Val f1: 0.8042440318302386\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.2693679302289922 - Tr acc: 0.8995247793618466 - Tr f1: 0.8994958286924002\n",
      "EVALUATION | Val Loss: 0.448899211982886 - Val acc: 0.8048780487804879 - Val f1: 0.8042440318302386\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.24567701811752013 - Tr acc: 0.9049558723693143 - Tr f1: 0.904923928077455\n",
      "EVALUATION | Val Loss: 0.4501817524433136 - Val acc: 0.8075880758807588 - Val f1: 0.8070211621894359\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.246665566358515 - Tr acc: 0.9022403258655805 - Tr f1: 0.9022024189354176\n",
      "EVALUATION | Val Loss: 0.44920337200164795 - Val acc: 0.8075880758807588 - Val f1: 0.8072256355542475\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.2454596848417354 - Tr acc: 0.9090291921249152 - Tr f1: 0.9090070071915914\n",
      "EVALUATION | Val Loss: 0.4493868077794711 - Val acc: 0.8048780487804879 - Val f1: 0.8045550847457628\n",
      "Executing model with lr=2e-06, warmup perc.=0.1, head=2_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6319744612580986 - Tr acc: 0.6503733876442634 - Tr f1: 0.6440715183551173\n",
      "EVALUATION | Val Loss: 0.5506761173407236 - Val acc: 0.7100271002710027 - Val f1: 0.7083477991091545\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.5353151857852936 - Tr acc: 0.736591989137814 - Tr f1: 0.7361394787092199\n",
      "EVALUATION | Val Loss: 0.5017911562075218 - Val acc: 0.7371273712737128 - Val f1: 0.7371273712737128\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.4682052122649326 - Tr acc: 0.7909029192124916 - Tr f1: 0.7903880983182406\n",
      "EVALUATION | Val Loss: 0.4639445810268323 - Val acc: 0.7777777777777778 - Val f1: 0.7773051106924165\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.39259881902766486 - Tr acc: 0.8221317040054311 - Tr f1: 0.8219799988929276\n",
      "EVALUATION | Val Loss: 0.45114788226783276 - Val acc: 0.7913279132791328 - Val f1: 0.7908301861790235\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.341961658850152 - Tr acc: 0.8479293957909029 - Tr f1: 0.8478434902587222\n",
      "EVALUATION | Val Loss: 0.44361269660294056 - Val acc: 0.7886178861788617 - Val f1: 0.788429873566598\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.3242139897199087 - Tr acc: 0.8662593346911066 - Tr f1: 0.8662294945937412\n",
      "EVALUATION | Val Loss: 0.4434877298772335 - Val acc: 0.7967479674796748 - Val f1: 0.7962631683561916\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.2991151151958332 - Tr acc: 0.8703326544467074 - Tr f1: 0.870246300647747\n",
      "EVALUATION | Val Loss: 0.4537353292107582 - Val acc: 0.7994579945799458 - Val f1: 0.7988063660477454\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.2803494050618141 - Tr acc: 0.8940936863543788 - Tr f1: 0.8940721563941717\n",
      "EVALUATION | Val Loss: 0.4503018781542778 - Val acc: 0.8021680216802168 - Val f1: 0.8015851385891383\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.27419622450746517 - Tr acc: 0.9008825526137135 - Tr f1: 0.9008583809699429\n",
      "EVALUATION | Val Loss: 0.45332650281488895 - Val acc: 0.8048780487804879 - Val f1: 0.8041170225315559\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.266247257910749 - Tr acc: 0.8940936863543788 - Tr f1: 0.8940193929733679\n",
      "EVALUATION | Val Loss: 0.4536229024330775 - Val acc: 0.8048780487804879 - Val f1: 0.8041170225315559\n",
      "Executing model with lr=2e-06, warmup perc.=0.05, head=2_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.5949939136864036 - Tr acc: 0.6856754921928038 - Tr f1: 0.6846003830122372\n",
      "EVALUATION | Val Loss: 0.5769261618455251 - Val acc: 0.7452574525745257 - Val f1: 0.7406848086124402\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.4983823943522669 - Tr acc: 0.7644263408010862 - Tr f1: 0.7637975501316341\n",
      "EVALUATION | Val Loss: 0.4851665465782086 - Val acc: 0.7696476964769647 - Val f1: 0.7693156326995918\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.4298747992243177 - Tr acc: 0.7970128988458928 - Tr f1: 0.7967397278860704\n",
      "EVALUATION | Val Loss: 0.4601282713313897 - Val acc: 0.7967479674796748 - Val f1: 0.7965327863428837\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.3756472900990517 - Tr acc: 0.8329938900203666 - Tr f1: 0.8328088780286141\n",
      "EVALUATION | Val Loss: 0.44553081567088765 - Val acc: 0.7831978319783198 - Val f1: 0.783119783707535\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.3318750740139074 - Tr acc: 0.8560760353021045 - Tr f1: 0.8560467766382335\n",
      "EVALUATION | Val Loss: 0.44041281876464683 - Val acc: 0.7994579945799458 - Val f1: 0.7992796236401059\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.281501510591116 - Tr acc: 0.8750848608282417 - Tr f1: 0.8749783198332011\n",
      "EVALUATION | Val Loss: 0.4521456602960825 - Val acc: 0.8021680216802168 - Val f1: 0.80169615053336\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.2678730180587179 - Tr acc: 0.8974881194840462 - Tr f1: 0.8974760229788524\n",
      "EVALUATION | Val Loss: 0.4502400979399681 - Val acc: 0.7994579945799458 - Val f1: 0.7992087769868815\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.2607739859370775 - Tr acc: 0.8940936863543788 - Tr f1: 0.8940526205133691\n",
      "EVALUATION | Val Loss: 0.45858090246717137 - Val acc: 0.8021680216802168 - Val f1: 0.8015851385891383\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.24648843889915814 - Tr acc: 0.9042769857433809 - Tr f1: 0.9042197752939566\n",
      "EVALUATION | Val Loss: 0.4598109057794015 - Val acc: 0.7994579945799458 - Val f1: 0.7988063660477454\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.24760285564648207 - Tr acc: 0.8974881194840462 - Tr f1: 0.8974510649186586\n",
      "EVALUATION | Val Loss: 0.460245824418962 - Val acc: 0.7994579945799458 - Val f1: 0.7988063660477454\n",
      "Executing model with lr=2e-06, warmup perc.=0.1, head=3_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6451616873664241 - Tr acc: 0.6286490156143923 - Tr f1: 0.6253607571541161\n",
      "EVALUATION | Val Loss: 0.5809062744180361 - Val acc: 0.6883468834688347 - Val f1: 0.688200327707444\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.5477442491439081 - Tr acc: 0.7399864222674813 - Tr f1: 0.739582664971046\n",
      "EVALUATION | Val Loss: 0.5221523828804493 - Val acc: 0.7506775067750677 - Val f1: 0.7486673773987207\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.4632109235371313 - Tr acc: 0.7827562797012899 - Tr f1: 0.7827273397743197\n",
      "EVALUATION | Val Loss: 0.48669996050496894 - Val acc: 0.7859078590785907 - Val f1: 0.7858826985934113\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.40482432178912625 - Tr acc: 0.8329938900203666 - Tr f1: 0.8328767350733879\n",
      "EVALUATION | Val Loss: 0.4806997459381819 - Val acc: 0.7913279132791328 - Val f1: 0.7909348441926346\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.3638241510557872 - Tr acc: 0.8438560760353021 - Tr f1: 0.8437776671093404\n",
      "EVALUATION | Val Loss: 0.45398350059986115 - Val acc: 0.8048780487804879 - Val f1: 0.8047044986768598\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.3312243574889757 - Tr acc: 0.8716904276985743 - Tr f1: 0.8716752870397557\n",
      "EVALUATION | Val Loss: 0.4527995493263006 - Val acc: 0.7967479674796748 - Val f1: 0.7966942148760331\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.3023467213716558 - Tr acc: 0.8893414799728445 - Tr f1: 0.8892514408673573\n",
      "EVALUATION | Val Loss: 0.4605923965573311 - Val acc: 0.7940379403794038 - Val f1: 0.7939153439153439\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.298432993952946 - Tr acc: 0.8947725729803123 - Tr f1: 0.8947677229609302\n",
      "EVALUATION | Val Loss: 0.4596541691571474 - Val acc: 0.7967479674796748 - Val f1: 0.7965327863428837\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.28754835816160323 - Tr acc: 0.8954514596062457 - Tr f1: 0.8953923995528714\n",
      "EVALUATION | Val Loss: 0.4592302218079567 - Val acc: 0.7913279132791328 - Val f1: 0.7912297846388973\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.27942241848476473 - Tr acc: 0.8981670061099797 - Tr f1: 0.8981027135715471\n",
      "EVALUATION | Val Loss: 0.45934695440034073 - Val acc: 0.7913279132791328 - Val f1: 0.7911745467106666\n",
      "Executing model with lr=2e-06, warmup perc.=0.05, head=3_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.6109959649783309 - Tr acc: 0.6687033265444671 - Tr f1: 0.6678422109103814\n",
      "EVALUATION | Val Loss: 0.5231995893021425 - Val acc: 0.7533875338753387 - Val f1: 0.7532715636641513\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.48996763379984004 - Tr acc: 0.7630685675492193 - Tr f1: 0.7628757844337892\n",
      "EVALUATION | Val Loss: 0.5186975002288818 - Val acc: 0.7669376693766937 - Val f1: 0.7645705976615824\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.4209337047030849 - Tr acc: 0.8092328581126952 - Tr f1: 0.8090466333243065\n",
      "EVALUATION | Val Loss: 0.45685214052597684 - Val acc: 0.8021680216802168 - Val f1: 0.8021622098019198\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.3583329983295933 - Tr acc: 0.8506449422946368 - Tr f1: 0.8506019075157298\n",
      "EVALUATION | Val Loss: 0.4526424426585436 - Val acc: 0.8075880758807588 - Val f1: 0.806901685620997\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.32775799361287905 - Tr acc: 0.8716904276985743 - Tr f1: 0.8716049781278752\n",
      "EVALUATION | Val Loss: 0.46066306717693806 - Val acc: 0.8021680216802168 - Val f1: 0.8011809425397645\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.29141715377248745 - Tr acc: 0.8968092328581126 - Tr f1: 0.8967840678591186\n",
      "EVALUATION | Val Loss: 0.4541987460106611 - Val acc: 0.8130081300813008 - Val f1: 0.8126558993414518\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.2615228757182116 - Tr acc: 0.9015614392396469 - Tr f1: 0.9015607133293666\n",
      "EVALUATION | Val Loss: 0.4533796397348245 - Val acc: 0.8157181571815718 - Val f1: 0.8154131355932204\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.24596059895170633 - Tr acc: 0.9124236252545825 - Tr f1: 0.9124234638030164\n",
      "EVALUATION | Val Loss: 0.4578899275511503 - Val acc: 0.8157181571815718 - Val f1: 0.8154891464203777\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.24695618654931745 - Tr acc: 0.911744738628649 - Tr f1: 0.9117193089865676\n",
      "EVALUATION | Val Loss: 0.46044517556826275 - Val acc: 0.8157181571815718 - Val f1: 0.8152282768777614\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.24011452744404474 - Tr acc: 0.9137813985064495 - Tr f1: 0.9137712246245978\n",
      "EVALUATION | Val Loss: 0.46060226671397686 - Val acc: 0.8157181571815718 - Val f1: 0.8152282768777614\n",
      "Executing model with lr=8e-06, warmup perc.=0.1, head=2_layers_S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.5650562051162925 - Tr acc: 0.7026476578411406 - Tr f1: 0.7020675094573827\n",
      "EVALUATION | Val Loss: 0.47980626362065476 - Val acc: 0.7886178861788617 - Val f1: 0.7869136460554371\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.3691697504211177 - Tr acc: 0.835030549898167 - Tr f1: 0.8349526560781664\n",
      "EVALUATION | Val Loss: 0.3988794330507517 - Val acc: 0.8184281842818428 - Val f1: 0.8183801652892562\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.21030436996971408 - Tr acc: 0.9226069246435845 - Tr f1: 0.9225966147946014\n",
      "EVALUATION | Val Loss: 0.5430902000516653 - Val acc: 0.7940379403794038 - Val f1: 0.7936970338983051\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.10280156394677056 - Tr acc: 0.96673455532926 - Tr f1: 0.9667306299732701\n",
      "EVALUATION | Val Loss: 0.6850464648644751 - Val acc: 0.8021680216802168 - Val f1: 0.8002669336002669\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.04095021097655458 - Tr acc: 0.9871011541072641 - Tr f1: 0.987099988799433\n",
      "EVALUATION | Val Loss: 0.779324687163656 - Val acc: 0.8157181571815718 - Val f1: 0.8156084656084657\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.024451704165067083 - Tr acc: 0.9925322471147319 - Tr f1: 0.992531902919808\n",
      "EVALUATION | Val Loss: 0.8748764460130284 - Val acc: 0.7994579945799458 - Val f1: 0.7989248895434463\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.008508036103379983 - Tr acc: 0.9979633401221996 - Tr f1: 0.997963280045537\n",
      "EVALUATION | Val Loss: 0.9369624780374579 - Val acc: 0.8075880758807588 - Val f1: 0.8070211621894359\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.005901042840779028 - Tr acc: 0.9986422267481331 - Tr f1: 0.9986421760580518\n",
      "EVALUATION | Val Loss: 0.9311446322826669 - Val acc: 0.8075880758807588 - Val f1: 0.8071291327105281\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.004358506637565311 - Tr acc: 0.9986422267481331 - Tr f1: 0.9986421960842151\n",
      "EVALUATION | Val Loss: 0.968960174000434 - Val acc: 0.8075880758807588 - Val f1: 0.806901685620997\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.0040143184058408264 - Tr acc: 0.9993211133740665 - Tr f1: 0.9993210933485124\n",
      "EVALUATION | Val Loss: 0.9700953976425808 - Val acc: 0.8075880758807588 - Val f1: 0.806901685620997\n",
      "Executing model with lr=8e-06, warmup perc.=0.05, head=2_layers_S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.5444340784383076 - Tr acc: 0.7155465037338764 - Tr f1: 0.7147466428180224\n",
      "EVALUATION | Val Loss: 0.4756436236202717 - Val acc: 0.7533875338753387 - Val f1: 0.7487748857183472\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.3288200384266274 - Tr acc: 0.8492871690427699 - Tr f1: 0.8492715385636533\n",
      "EVALUATION | Val Loss: 0.43205369512240094 - Val acc: 0.7831978319783198 - Val f1: 0.782736693358455\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.16890778773093737 - Tr acc: 0.9395790902919212 - Tr f1: 0.9395780877755517\n",
      "EVALUATION | Val Loss: 0.5235966785500447 - Val acc: 0.8157181571815718 - Val f1: 0.8156518161514048\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.06010874667234959 - Tr acc: 0.9809911744738629 - Tr f1: 0.9809896937627907\n",
      "EVALUATION | Val Loss: 0.7760758923056225 - Val acc: 0.8021680216802168 - Val f1: 0.8010223302333483\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.02004846620125576 - Tr acc: 0.9938900203665988 - Tr f1: 0.9938899189885388\n",
      "EVALUATION | Val Loss: 0.9035421864440044 - Val acc: 0.8211382113821138 - Val f1: 0.8205570291777189\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.014214858766995691 - Tr acc: 0.9972844534962662 - Tr f1: 0.9972843921684305\n",
      "EVALUATION | Val Loss: 1.020609810637931 - Val acc: 0.7994579945799458 - Val f1: 0.7978411513859276\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.004781026166290426 - Tr acc: 0.9986422267481331 - Tr f1: 0.9986422111034501\n",
      "EVALUATION | Val Loss: 1.0114211138958733 - Val acc: 0.8130081300813008 - Val f1: 0.8124571857897334\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.0012977925296347107 - Tr acc: 1.0 - Tr f1: 1.0\n",
      "EVALUATION | Val Loss: 1.0598123902454972 - Val acc: 0.8021680216802168 - Val f1: 0.8013275804845669\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.0010782618066846764 - Tr acc: 1.0 - Tr f1: 1.0\n",
      "EVALUATION | Val Loss: 1.0440466584016879 - Val acc: 0.8102981029810298 - Val f1: 0.8096816976127321\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.0050646864814728855 - Tr acc: 0.9993211133740665 - Tr f1: 0.9993211021098376\n",
      "EVALUATION | Val Loss: 1.048369264851014 - Val acc: 0.8075880758807588 - Val f1: 0.806901685620997\n",
      "Executing model with lr=8e-06, warmup perc.=0.1, head=2_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.5568879431373971 - Tr acc: 0.6897488119484046 - Tr f1: 0.6886375252372241\n",
      "EVALUATION | Val Loss: 0.5564392606417338 - Val acc: 0.7452574525745257 - Val f1: 0.741411701556629\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.4230003589263526 - Tr acc: 0.8092328581126952 - Tr f1: 0.8091058140419733\n",
      "EVALUATION | Val Loss: 0.42045852914452553 - Val acc: 0.7967479674796748 - Val f1: 0.7966942148760332\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.22465942659845917 - Tr acc: 0.9192124915139172 - Tr f1: 0.9192051930069742\n",
      "EVALUATION | Val Loss: 0.48653090589990217 - Val acc: 0.7994579945799458 - Val f1: 0.7980325443786983\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.10052395984280094 - Tr acc: 0.9680923285811269 - Tr f1: 0.9680920932860706\n",
      "EVALUATION | Val Loss: 0.8064671144626724 - Val acc: 0.7886178861788617 - Val f1: 0.7857079039847519\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.05518506639274037 - Tr acc: 0.9803122878479293 - Tr f1: 0.9803086576540487\n",
      "EVALUATION | Val Loss: 0.797111705566446 - Val acc: 0.8102981029810298 - Val f1: 0.8102298107440931\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.020686735702310537 - Tr acc: 0.9959266802443992 - Tr f1: 0.9959265281741555\n",
      "EVALUATION | Val Loss: 0.8455787982869273 - Val acc: 0.8211382113821138 - Val f1: 0.8210738215587163\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.01064889572100133 - Tr acc: 0.9959266802443992 - Tr f1: 0.9959265281741555\n",
      "EVALUATION | Val Loss: 0.9105729918228462 - Val acc: 0.8130081300813008 - Val f1: 0.8126558993414518\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.006695664051534628 - Tr acc: 0.9972844534962662 - Tr f1: 0.9972844222069002\n",
      "EVALUATION | Val Loss: 0.9034608963217275 - Val acc: 0.8130081300813008 - Val f1: 0.8129201966244664\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.004227672982319403 - Tr acc: 0.9993211133740665 - Tr f1: 0.9993210933485124\n",
      "EVALUATION | Val Loss: 0.9373212892678566 - Val acc: 0.8048780487804879 - Val f1: 0.804359351988218\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.003461731349929206 - Tr acc: 0.9986422267481331 - Tr f1: 0.9986421960842151\n",
      "EVALUATION | Val Loss: 0.936749687534757 - Val acc: 0.8048780487804879 - Val f1: 0.804359351988218\n",
      "Executing model with lr=8e-06, warmup perc.=0.05, head=2_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.521353700328418 - Tr acc: 0.7406653088934148 - Tr f1: 0.7400613852897046\n",
      "EVALUATION | Val Loss: 0.4482702699800332 - Val acc: 0.7804878048780488 - Val f1: 0.777321661389458\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.32147583910214006 - Tr acc: 0.8655804480651731 - Tr f1: 0.8655665073675827\n",
      "EVALUATION | Val Loss: 0.5017676247904698 - Val acc: 0.7886178861788617 - Val f1: 0.7838150462684774\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.1431509898612476 - Tr acc: 0.9531568228105907 - Tr f1: 0.9531560455787986\n",
      "EVALUATION | Val Loss: 0.6368766020362576 - Val acc: 0.8102981029810298 - Val f1: 0.8099841101694916\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.06592113681460521 - Tr acc: 0.9816700610997964 - Tr f1: 0.9816695204098329\n",
      "EVALUATION | Val Loss: 0.8441228354349732 - Val acc: 0.8102981029810298 - Val f1: 0.8079268292682926\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.02144931876588793 - Tr acc: 0.9925322471147319 - Tr f1: 0.9925320268336356\n",
      "EVALUATION | Val Loss: 1.004180615923057 - Val acc: 0.8075880758807588 - Val f1: 0.8045507105822671\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.009365391600201087 - Tr acc: 0.9972844534962662 - Tr f1: 0.9972843921684305\n",
      "EVALUATION | Val Loss: 0.8621439946582541 - Val acc: 0.8102981029810298 - Val f1: 0.8096816976127321\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.004457288127522716 - Tr acc: 0.9979633401221996 - Tr f1: 0.9979632462508568\n",
      "EVALUATION | Val Loss: 0.9670307264314033 - Val acc: 0.8211382113821138 - Val f1: 0.8195144364735876\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.000880628917491444 - Tr acc: 1.0 - Tr f1: 1.0\n",
      "EVALUATION | Val Loss: 1.0131779323225298 - Val acc: 0.8211382113821138 - Val f1: 0.8193216214612142\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.0006446213012696394 - Tr acc: 1.0 - Tr f1: 1.0\n",
      "EVALUATION | Val Loss: 0.9623104148631683 - Val acc: 0.8184281842818428 - Val f1: 0.8173766592552649\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.000980121565022586 - Tr acc: 1.0 - Tr f1: 1.0\n",
      "EVALUATION | Val Loss: 0.9703466180168713 - Val acc: 0.8238482384823849 - Val f1: 0.8225134493143995\n",
      "Executing model with lr=8e-06, warmup perc.=0.1, head=3_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.5812395730967163 - Tr acc: 0.6897488119484046 - Tr f1: 0.6896664274027697\n",
      "EVALUATION | Val Loss: 0.480507114281257 - Val acc: 0.7615176151761518 - Val f1: 0.7595948827292112\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.3971546790612641 - Tr acc: 0.835030549898167 - Tr f1: 0.8349937420623303\n",
      "EVALUATION | Val Loss: 0.522576275592049 - Val acc: 0.7479674796747967 - Val f1: 0.7368025463051732\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.2417513112267179 - Tr acc: 0.9097080787508486 - Tr f1: 0.9097039171213143\n",
      "EVALUATION | Val Loss: 0.5049116446947058 - Val acc: 0.8048780487804879 - Val f1: 0.8038277511961722\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.11185683688068743 - Tr acc: 0.9633401221995926 - Tr f1: 0.9633380776619556\n",
      "EVALUATION | Val Loss: 0.7877497992788752 - Val acc: 0.7859078590785907 - Val f1: 0.7819034722170267\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.056356537051909475 - Tr acc: 0.9864222674813307 - Tr f1: 0.9864215102451688\n",
      "EVALUATION | Val Loss: 1.0403822505419764 - Val acc: 0.7588075880758808 - Val f1: 0.7518006000740645\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.04022875356472908 - Tr acc: 0.991174473862865 - Tr f1: 0.9911743274278896\n",
      "EVALUATION | Val Loss: 0.9231852835800964 - Val acc: 0.7859078590785907 - Val f1: 0.7836130555163787\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.022817764180882643 - Tr acc: 0.9966055668703326 - Tr f1: 0.9966055105491882\n",
      "EVALUATION | Val Loss: 0.8843729546448836 - Val acc: 0.8102981029810298 - Val f1: 0.8101293737136136\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.017361504215088683 - Tr acc: 0.9972844534962662 - Tr f1: 0.9972843921684305\n",
      "EVALUATION | Val Loss: 0.9176351961214095 - Val acc: 0.7967479674796748 - Val f1: 0.7961491149888407\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.01323921139782635 - Tr acc: 0.9979633401221996 - Tr f1: 0.9979633063295129\n",
      "EVALUATION | Val Loss: 0.9128903343031803 - Val acc: 0.8102981029810298 - Val f1: 0.8099841101694916\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.014009612903464586 - Tr acc: 0.9986422267481331 - Tr f1: 0.9986421960842151\n",
      "EVALUATION | Val Loss: 0.9116586983048668 - Val acc: 0.8130081300813008 - Val f1: 0.8127385724267274\n",
      "Executing model with lr=8e-06, warmup perc.=0.05, head=3_layers_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Epoch: 0\n",
      "TRAINING | Tr Loss: 0.5394223152950246 - Tr acc: 0.7372708757637475 - Tr f1: 0.7370959075951731\n",
      "EVALUATION | Val Loss: 0.4668655215452115 - Val acc: 0.7886178861788617 - Val f1: 0.7877934410758524\n",
      "Epoch: 1\n",
      "TRAINING | Tr Loss: 0.33192866263530585 - Tr acc: 0.8608282416836388 - Tr f1: 0.8608218271418754\n",
      "EVALUATION | Val Loss: 0.4620110293229421 - Val acc: 0.7859078590785907 - Val f1: 0.7855045804054303\n",
      "Epoch: 2\n",
      "TRAINING | Tr Loss: 0.1586220611327438 - Tr acc: 0.9477257298031229 - Tr f1: 0.9477241878354492\n",
      "EVALUATION | Val Loss: 0.6451182886958122 - Val acc: 0.7886178861788617 - Val f1: 0.7866988794687853\n",
      "Epoch: 3\n",
      "TRAINING | Tr Loss: 0.08703274315633681 - Tr acc: 0.9735234215885947 - Tr f1: 0.9735232263437607\n",
      "EVALUATION | Val Loss: 0.842063651420176 - Val acc: 0.8102981029810298 - Val f1: 0.8102855631940772\n",
      "Epoch: 4\n",
      "TRAINING | Tr Loss: 0.035123024228459566 - Tr acc: 0.991174473862865 - Tr f1: 0.9911743274278896\n",
      "EVALUATION | Val Loss: 0.9728441943104068 - Val acc: 0.7967479674796748 - Val f1: 0.7961491149888407\n",
      "Epoch: 5\n",
      "TRAINING | Tr Loss: 0.01961708820790493 - Tr acc: 0.9959266802443992 - Tr f1: 0.9959265882526456\n",
      "EVALUATION | Val Loss: 1.014040466863662 - Val acc: 0.7886178861788617 - Val f1: 0.7876431354031401\n",
      "Epoch: 6\n",
      "TRAINING | Tr Loss: 0.013686281385435973 - Tr acc: 0.9986422267481331 - Tr f1: 0.9986421760580518\n",
      "EVALUATION | Val Loss: 1.04208933437864 - Val acc: 0.8157181571815718 - Val f1: 0.8156843156843157\n",
      "Epoch: 7\n",
      "TRAINING | Tr Loss: 0.010808242355055247 - Tr acc: 0.9986422267481331 - Tr f1: 0.9986421760580518\n",
      "EVALUATION | Val Loss: 1.0478962832906593 - Val acc: 0.7967479674796748 - Val f1: 0.7960229073461236\n",
      "Epoch: 8\n",
      "TRAINING | Tr Loss: 0.007964824489380424 - Tr acc: 0.9979633401221996 - Tr f1: 0.997963280045537\n",
      "EVALUATION | Val Loss: 1.0760445372046281 - Val acc: 0.7913279132791328 - Val f1: 0.7902867476104365\n",
      "Epoch: 9\n",
      "TRAINING | Tr Loss: 0.0060009799829761805 - Tr acc: 0.9993211133740665 - Tr f1: 0.9993210933485124\n",
      "EVALUATION | Val Loss: 1.0630071075089897 - Val acc: 0.7967479674796748 - Val f1: 0.7960229073461236\n"
     ]
    }
   ],
   "source": [
    "num_trials = len(lr_list) * len(warmup_perc_list) * len(clf_head_list)\n",
    "print('Executing {} trials in grid search'.format(num_trials))\n",
    "\n",
    "\n",
    "for lr in lr_list:\n",
    "    for clf_head_tuple in clf_head_list:\n",
    "        for warmup_perc in warmup_perc_list:\n",
    "            \n",
    "            # Model save path subfolder\n",
    "            model_folder_str = 'lr{}_warmup{}_head_{}'.format(lr, warmup_perc, clf_head_tuple[1])\n",
    "            model_folder_pth = os.path.join(save_folder_pth, model_folder_str)\n",
    "            if not os.path.exists(model_folder_pth):\n",
    "                os.makedirs(model_folder_pth)\n",
    "\n",
    "            print('Executing model with lr={}, warmup perc.={}, head={}'.format(lr, warmup_perc, clf_head_tuple[1]))\n",
    "\n",
    "            train_eval_model(lr, warmup_steps=warmup_perc*tr_steps, head=clf_head_tuple[0], model_folder_pth=model_folder_pth)\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate = 0.5e-5\n",
    "scheduler_warmup_step = 0\n",
    "max_epochs = 12\n",
    "\n",
    "# Num batches*num epochs\n",
    "tr_steps = len(tr_dataloader)*max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_clf_model, loss_function, optimizer, scheduler, device = init_bert_clf(tr_steps=tr_steps, lr_rate=lr_rate, scheduler_warmp_steps=scheduler_warmup_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using as device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('Using as device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    6546 KB |   11833 MB |  760866 GB |  760866 GB |\n",
      "|       from large pool |    4096 KB |   11827 MB |  759221 GB |  759221 GB |\n",
      "|       from small pool |    2450 KB |      16 MB |    1644 GB |    1644 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    6546 KB |   11833 MB |  760866 GB |  760866 GB |\n",
      "|       from large pool |    4096 KB |   11827 MB |  759221 GB |  759221 GB |\n",
      "|       from small pool |    2450 KB |      16 MB |    1644 GB |    1644 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   47104 KB |   12174 MB |  218498 MB |  218452 MB |\n",
      "|       from large pool |   40960 KB |   12156 MB |  218288 MB |  218248 MB |\n",
      "|       from small pool |    6144 KB |      18 MB |     210 MB |     204 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   40558 KB |  456148 KB |  193015 GB |  193015 GB |\n",
      "|       from large pool |   36864 KB |  454762 KB |  191343 GB |  191343 GB |\n",
      "|       from small pool |    3694 KB |    4139 KB |    1671 GB |    1671 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      28    |    1083    |   30698 K  |   30698 K  |\n",
      "|       from large pool |       2    |     486    |   15602 K  |   15602 K  |\n",
      "|       from small pool |      26    |     742    |   15096 K  |   15096 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      28    |    1083    |   30698 K  |   30698 K  |\n",
      "|       from large pool |       2    |     486    |   15602 K  |   15602 K  |\n",
      "|       from small pool |      26    |     742    |   15096 K  |   15096 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       5    |     201    |    3535    |    3530    |\n",
      "|       from large pool |       2    |     192    |    3430    |    3428    |\n",
      "|       from small pool |       3    |       9    |     105    |     102    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      14    |      90    |   10491 K  |   10491 K  |\n",
      "|       from large pool |       2    |      72    |    4030 K  |    4030 K  |\n",
      "|       from small pool |      12    |      22    |    6461 K  |    6461 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-08 06:54:58.724348\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "curr_date = datetime.now()\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "save_folder_pth = './bert_subtaskA/{}_{}_{}-{}.{}'.format(curr_date.day, curr_date.month, curr_date.day, curr_date.hour, curr_date.minute)\n",
    "if(not os.path.exists(save_folder_pth)):\n",
    "    os.makedirs(save_folder_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_optimizer = optimizer\n",
    "curr_scheduler = scheduler\n",
    "curr_bert_clf = bert_clf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Tr Loss: 0.6081453029827405 - Tr acc: 0.6687033265444671 - Tr f1: 0.6687031738540499\n",
      "EVALUATION | Val Loss: 0.5338722169399261 - Val acc: 0.7235772357723578 - Val f1: 0.7232337196305666\n",
      "Epoch: 1| Tr Loss: 0.4911935066664091 - Tr acc: 0.7766463000678887 - Tr f1: 0.7763886511300837\n",
      "EVALUATION | Val Loss: 0.48243209905922413 - Val acc: 0.7560975609756098 - Val f1: 0.7555787800282618\n",
      "Epoch: 2| Tr Loss: 0.35770584442602693 - Tr acc: 0.8418194161575017 - Tr f1: 0.841735094921868\n",
      "EVALUATION | Val Loss: 0.5237541813403368 - Val acc: 0.7831978319783198 - Val f1: 0.781449893390192\n",
      "Epoch: 3| Tr Loss: 0.2988268958945428 - Tr acc: 0.8940936863543788 - Tr f1: 0.8940760627058204\n",
      "EVALUATION | Val Loss: 0.5829353736092647 - Val acc: 0.7831978319783198 - Val f1: 0.7818504286136565\n",
      "Epoch: 4| Tr Loss: 0.17743003027393453 - Tr acc: 0.9436524100475221 - Tr f1: 0.9436398378237589\n",
      "EVALUATION | Val Loss: 0.813459321235617 - Val acc: 0.7669376693766937 - Val f1: 0.763419216317767\n",
      "Epoch: 5| Tr Loss: 0.1293995042521787 - Tr acc: 0.9592668024439919 - Tr f1: 0.9592645307355061\n",
      "EVALUATION | Val Loss: 0.853834617882967 - Val acc: 0.7886178861788617 - Val f1: 0.7876431354031401\n",
      "Epoch: 6| Tr Loss: 0.09085456618878951 - Tr acc: 0.9755600814663951 - Tr f1: 0.9755595295158733\n",
      "EVALUATION | Val Loss: 0.8977032753949364 - Val acc: 0.7831978319783198 - Val f1: 0.783119783707535\n",
      "Epoch: 7| Tr Loss: 0.09075262734005528 - Tr acc: 0.9708078750848609 - Tr f1: 0.9708065295956129\n",
      "EVALUATION | Val Loss: 1.0287648656715949 - Val acc: 0.7750677506775068 - Val f1: 0.7726567545298663\n",
      "Epoch: 8| Tr Loss: 0.06494239470370675 - Tr acc: 0.9789545145960624 - Tr f1: 0.9789538938038822\n",
      "EVALUATION | Val Loss: 1.0391969146827857 - Val acc: 0.7777777777777778 - Val f1: 0.7757603604671844\n",
      "Epoch: 9| Tr Loss: 0.051693449778262006 - Tr acc: 0.988458927359131 - Tr f1: 0.9884583954215214\n",
      "EVALUATION | Val Loss: 1.1341218358526628 - Val acc: 0.7560975609756098 - Val f1: 0.7520753702818919\n",
      "Epoch: 10| Tr Loss: 0.05058879248537524 - Tr acc: 0.988458927359131 - Tr f1: 0.9884585869247096\n",
      "EVALUATION | Val Loss: 1.1243657985081275 - Val acc: 0.7669376693766937 - Val f1: 0.7637292274703675\n",
      "Epoch: 11| Tr Loss: 0.04941885238413208 - Tr acc: 0.988458927359131 - Tr f1: 0.9884583954215214\n",
      "EVALUATION | Val Loss: 1.118853572756052 - Val acc: 0.7696476964769647 - Val f1: 0.7666242550056921\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(max_epochs):\n",
    "    # Train\n",
    "    avg_epoch_loss_tr, acc_score_tr, f1_score_tr, model, optimizer, scheduler = train_bert_clf(bert_clf_model, tr_dataloader, loss_function, optimizer, scheduler, device)\n",
    "    curr_optimizer = optimizer\n",
    "    curr_scheduler = scheduler\n",
    "    curr_bert_clf = model\n",
    "    \n",
    "    print('Epoch: {}| Tr Loss: {} - Tr acc: {} - Tr f1: {}'.format(epoch_i, avg_epoch_loss_tr, acc_score_tr, f1_score_tr))\n",
    "\n",
    "    # Eval\n",
    "    avg_epoch_loss_val, acc_score_val, f1_score_val, predictions, labels = eval_bert_clf(bert_clf_model, val_dataloader, loss_function, device)\n",
    "    print('EVALUATION | Val Loss: {} - Val acc: {} - Val f1: {}'.format(avg_epoch_loss_val, acc_score_val, f1_score_val))\n",
    "\n",
    "    # Save\n",
    "    save_path = os.path.join(save_folder_pth, 'bert_clf.pt')\n",
    "    torch.save({\n",
    "                'epoch': epoch_i,\n",
    "                'model_state_dict': curr_bert_clf.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'schdeduler_state_dict': scheduler.state_dict(),\n",
    "                'tr_loss': avg_epoch_loss_tr,\n",
    "                'val_loss': avg_epoch_loss_val,\n",
    "                'tr_acc': acc_score_tr,\n",
    "                'val_acc': acc_score_val,\n",
    "                'tr_f1': f1_score_tr,\n",
    "                'val_f1': f1_score_val,\n",
    "                'val_preds': predictions\n",
    "                }, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
