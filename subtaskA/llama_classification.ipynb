{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from utils import get_device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_csv('subtaskA_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>âš¡Se non ci fossero soldati non ci sarebbero gu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>21/08/21]( [PRE-PRINT]\\n\\nðŸ“„__ \"Shedding of Inf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>L'Aspirina non aumenta la sopravvivenza dei pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>L'Italia non puo' dare armi lo vieta la Costit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                       comment_text  conspiratorial\n",
       "0   0  âš¡Se non ci fossero soldati non ci sarebbero gu...               0\n",
       "1   1  21/08/21]( [PRE-PRINT]\\n\\nðŸ“„__ \"Shedding of Inf...               1\n",
       "2   2  PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...               1\n",
       "3   3  L'Aspirina non aumenta la sopravvivenza dei pa...               0\n",
       "4   4  L'Italia non puo' dare armi lo vieta la Costit...               0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1842 entries, 0 to 1841\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Id              1842 non-null   int64 \n",
      " 1   comment_text    1842 non-null   object\n",
      " 2   conspiratorial  1842 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 43.3+ KB\n"
     ]
    }
   ],
   "source": [
    "texts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    925\n",
       "0    917\n",
       "Name: conspiratorial, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.conspiratorial.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>La CNN ha appena ammesso che Trump non Ã¨ piÃ¹ a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Notizia del 2017: â€œAutovelox, la foto viola la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>La foto di due \"sospetti\" in un palazzo non Ã¨ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>La falsa notizia della bambina con il cartello...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ah dimenticavo.. e' gratuita</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                       comment_text\n",
       "0   0  La CNN ha appena ammesso che Trump non Ã¨ piÃ¹ a...\n",
       "1   1  Notizia del 2017: â€œAutovelox, la foto viola la...\n",
       "2   2  La foto di due \"sospetti\" in un palazzo non Ã¨ ...\n",
       "3   3  La falsa notizia della bambina con il cartello...\n",
       "4   4                       Ah dimenticavo.. e' gratuita"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df_test = pd.read_csv('subtaskA_test.csv')\n",
    "texts_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 460 entries, 0 to 459\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Id            460 non-null    int64 \n",
      " 1   comment_text  460 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 7.3+ KB\n"
     ]
    }
   ],
   "source": [
    "texts_df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove break line characthers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.comment_text = texts_df.comment_text.apply(lambda text: text.replace('\\n\\n', ' ').replace('\\n', ' '))\n",
    "texts_df_test.comment_text = texts_df_test.comment_text.apply(lambda text: str(text).replace('\\n\\n', ' ').replace('\\n', ' '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80-20 train-validation split of the labeled tr dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing datasets using stratified sampling\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, eval_index in split.split(texts_df, texts_df.conspiratorial):\n",
    "    train_df, val_df = texts_df.iloc[train_index], texts_df.iloc[eval_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1473 entries, 1509 to 1768\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Id              1473 non-null   int64 \n",
      " 1   comment_text    1473 non-null   object\n",
      " 2   conspiratorial  1473 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 46.0+ KB\n",
      "None\n",
      "1    740\n",
      "0    733\n",
      "Name: conspiratorial, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())\n",
    "print(train_df.conspiratorial.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 369 entries, 361 to 667\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Id              369 non-null    int64 \n",
      " 1   comment_text    369 non-null    object\n",
      " 2   conspiratorial  369 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 11.5+ KB\n",
      "None\n",
      "1    185\n",
      "0    184\n",
      "Name: conspiratorial, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(val_df.info())\n",
    "print(val_df.conspiratorial.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('/home/giacomo/Ai/llama/hgf_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tr = train_df['comment_text']\n",
    "labels_tr = train_df['conspiratorial'].to_numpy()\n",
    "\n",
    "texts_val = val_df['comment_text']\n",
    "labels_val = val_df['conspiratorial'].to_numpy()\n",
    "\n",
    "texts_test = texts_df_test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_len_list = []\n",
    "\n",
    "for sentence in texts_tr:\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    tokenized_len_list.append(len(input_ids))\n",
    "for sentence in texts_val:\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    tokenized_len_list.append(len(input_ids))\n",
    "\n",
    "sns.displot(tokenized_len_list)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama max token length is 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tr, input_ids_val, input_ids_test = [], [], []\n",
    "\n",
    "for sentence in texts_tr:\n",
    "    tokenized = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    input_ids_tr.append(tokenized)\n",
    "\n",
    "for sentence in texts_val:\n",
    "    tokenized = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    input_ids_val.append(tokenized)\n",
    "\n",
    "for sentence in texts_test:\n",
    "    tokenized = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    input_ids_test.append(tokenized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.48s/it]\n",
      "Some weights of the model checkpoint at /home/giacomo/Ai/llama/hgf_weights were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaModel\n",
    "\n",
    "model = LlamaModel.from_pretrained('/home/giacomo/Ai/llama/hgf_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_embeddings_tr, llama_embeddings_val, llama_embeddings_test = [], [], []\n",
    "\n",
    "llama_embed_folder_pth = 'llama_embeddings'\n",
    "if not os.path.exists(llama_embed_folder_pth):\n",
    "    os.makedirs(llama_embed_folder_pth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get and save Llama embeddings for traning, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1473/1473 [1:51:43<00:00,  4.55s/it]  \n"
     ]
    }
   ],
   "source": [
    "for input_ids in tqdm(input_ids_tr):\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)\n",
    "        # Taking only the last token : [ all sentences (:), only the last position (-1), all hidden unit outputs (:) ]\n",
    "        llama_embeddings_tr.append(last_hidden_states[0][:, -1, :].numpy())\n",
    "\n",
    "\n",
    "# Saving the embeddings\n",
    "np.save(os.path.join(llama_embed_folder_pth, 'llama_embeddings_tr.npy'), llama_embeddings_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [26:40<00:00,  4.34s/it] \n"
     ]
    }
   ],
   "source": [
    "for input_ids in tqdm(input_ids_val):\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)\n",
    "        # Taking only the last token : [ all sentences (:), only the last position (-1), all hidden unit outputs (:) ]\n",
    "        llama_embeddings_val.append(last_hidden_states[0][:, -1, :].numpy())\n",
    "\n",
    "# Saving the embeddings\n",
    "np.save(os.path.join(llama_embed_folder_pth, 'llama_embeddings_val.npy'), llama_embeddings_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460/460 [38:56<00:00,  5.08s/it] \n"
     ]
    }
   ],
   "source": [
    "for input_ids in tqdm(input_ids_test):\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)\n",
    "        # Taking only the last token : [ all sentences (:), only the last position (-1), all hidden unit outputs (:) ]\n",
    "        llama_embeddings_test.append(last_hidden_states[0][:, -1, :].numpy())\n",
    "\n",
    "# Saving the embeddings\n",
    "np.save(os.path.join(llama_embed_folder_pth, 'llama_embeddings_test.npy'), llama_embeddings_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN classifier on Llama embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tr = torch.tensor(labels_tr)\n",
    "labels_val = torch.tensor(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_embeddings_tr = torch.tensor(np.load(os.path.join(llama_embed_folder_pth, 'llama_embeddings_tr.npy')))\n",
    "llama_embeddings_val = torch.tensor(np.load(os.path.join(llama_embed_folder_pth, 'llama_embeddings_val.npy')))\n",
    "llama_embeddings_test = torch.tensor(np.load(os.path.join(llama_embed_folder_pth, 'llama_embeddings_test.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap data into a TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = TensorDataset(llama_embeddings_tr, labels_tr)\n",
    "val_dataset = TensorDataset(llama_embeddings_val, labels_val)\n",
    "test_dataset = TensorDataset(input_ids_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "tr_dataloader = DataLoader(tr_dataset, sampler=RandomSampler(tr_dataset), batch_size = batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Convert the lists into tensors\n",
    "\n",
    "input_ids_tr = torch.cat(input_ids_tr, dim=0)\n",
    "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "\n",
    "attention_masks_tr = torch.cat(attention_masks_tr, dim=0)\n",
    "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "\n",
    "labels_tr = torch.tensor(labels_tr)\n",
    "labels_val = torch.tensor(labels_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tr_dataset = TensorDataset(input_ids_tr, attention_masks_tr, labels_tr)\n",
    "val_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLoader needs to know our batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DataLoaders for our training and validation sets. Tr samples are taken in random order, while validation are taken sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tr_dataloader = DataLoader(tr_dataset, sampler=RandomSampler(tr_dataset), batch_size = batch_size)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size = batch_size)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size = batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set save folder for this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "curr_date = datetime.now()\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "save_folder_pth = './checkpoints_bert_subtaskA/{}_{}_{}-{}.{}'.format(curr_date.day, curr_date.month, curr_date.year, curr_date.hour, curr_date.minute)\n",
    "if not os.path.exists(save_folder_pth):\n",
    "    os.makedirs(save_folder_pth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed general hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "max_epochs = 12\n",
    "\n",
    "\n",
    "# Num batches*num epochs\n",
    "tr_steps = len(tr_dataloader)*max_epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable grid searched hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "lr_list = [1e-6, 2e-6, 3e-6]\n",
    "warmup_perc_list = [0.1, 0.05] # Percentage of warmup steps for scheduler on the total tr steps\n",
    "clf_head_list = []\n",
    "\n",
    "head1 = nn.Sequential(\n",
    "                nn.Linear(768, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25),\n",
    "                nn.Linear(128, 2),\n",
    "                )\n",
    "clf_head_list.append((head1, '2_layers_S')) # Each head obeject is composed by a tuple (head, name)\n",
    "\n",
    "head2 = nn.Sequential(\n",
    "                nn.Linear(768, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25),\n",
    "                nn.Linear(256, 2),\n",
    "                )\n",
    "clf_head_list.append((head2, '2_layers_M'))\n",
    "\n",
    "head3 = nn.Sequential(\n",
    "                nn.Linear(768, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 2),\n",
    "                )\n",
    "clf_head_list.append((head3, '3_layers_M'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize, train/eval and save function for each grid search run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def train_eval_model(lr, warmup_steps, head, model_folder_pth, device):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    #print(torch.cuda.memory_summary())\n",
    "\n",
    "    # Initialize model\n",
    "    bert_clf_model, loss_function, optimizer, scheduler = init_bert_clf(tr_steps=tr_steps, lr_rate=lr, scheduler_warmp_steps=warmup_steps, head=head)\n",
    "\n",
    "    for epoch_i in range(max_epochs):\n",
    "        print('Epoch: {}'.format(epoch_i))\n",
    "\n",
    "        # Train\n",
    "        avg_epoch_loss_tr, acc_score_tr, f1_score_tr, bert_clf_model, optimizer, scheduler = train_clf(bert_clf_model, tr_dataloader, loss_function, optimizer, scheduler, device)\n",
    "\n",
    "\n",
    "        print('TRAINING | Tr Loss: {} - Tr acc: {} - Tr f1: {}'.format(avg_epoch_loss_tr, acc_score_tr, f1_score_tr))\n",
    "\n",
    "        # Eval\n",
    "        avg_epoch_loss_val, acc_score_val, f1_score_val, predictions, labels = eval_clf(bert_clf_model, val_dataloader, loss_function, device)\n",
    "        print('EVALUATION | Val Loss: {} - Val acc: {} - Val f1: {}'.format(avg_epoch_loss_val, acc_score_val, f1_score_val))\n",
    "\n",
    "        # Save\n",
    "        model_save_pth = os.path.join(model_folder_pth, 'bert_clf_{}.pt'.format(epoch_i))\n",
    "        torch.save({\n",
    "                    'epoch': epoch_i,\n",
    "                    'model_state_dict': bert_clf_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'schdeduler_state_dict': scheduler.state_dict(),\n",
    "                    'tr_loss': avg_epoch_loss_tr,\n",
    "                    'val_loss': avg_epoch_loss_val,\n",
    "                    'tr_acc': acc_score_tr,\n",
    "                    'val_acc': acc_score_val,\n",
    "                    'tr_f1': f1_score_tr,\n",
    "                    'val_f1': f1_score_val,\n",
    "                    'val_preds': predictions\n",
    "                    }, model_save_pth)\n",
    "\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "num_trials = len(lr_list) * len(warmup_perc_list) * len(clf_head_list)\n",
    "print('Executing {} trials in grid search'.format(num_trials))\n",
    "\n",
    "\n",
    "for lr in lr_list:\n",
    "    for clf_head_tuple in clf_head_list:\n",
    "        for warmup_perc in warmup_perc_list:\n",
    "            \n",
    "            # Model save path subfolder\n",
    "            model_folder_str = 'lr{}_warmup{}_head_{}'.format(lr, warmup_perc, clf_head_tuple[1])\n",
    "            model_folder_pth = os.path.join(save_folder_pth, model_folder_str)\n",
    "            if not os.path.exists(model_folder_pth):\n",
    "                os.makedirs(model_folder_pth)\n",
    "\n",
    "            print()\n",
    "            print('Executing model with lr={}, warmup perc.={}, head={}'.format(lr, warmup_perc, clf_head_tuple[1]))\n",
    "\n",
    "            train_eval_model(lr, warmup_steps=warmup_perc*tr_steps, head=clf_head_tuple[0], model_folder_pth=model_folder_pth, device=device)\n",
    "\n",
    "           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "save_folder_pth = './checkpoints_bert_subtaskA/16_4_2023-5.1' # Grid search run to analyze\n",
    "\n",
    "\n",
    "best_f1_list = []\n",
    "\n",
    "for lr in lr_list:\n",
    "    for clf_head_tuple in clf_head_list:\n",
    "        for warmup_perc in warmup_perc_list:\n",
    "            \n",
    "            # Model path subfolder\n",
    "            model_folder_str = 'lr{}_warmup{}_head_{}'.format(lr, warmup_perc, clf_head_tuple[1])\n",
    "            model_folder_pth = os.path.join(save_folder_pth, model_folder_str)\n",
    "\n",
    "            title = 'BERT with lr={}, warmup perc.={}, head={}'.format(lr, warmup_perc, clf_head_tuple[1])\n",
    "\n",
    "            # Load model checkpoint for each epoch\n",
    "            val_loss_list, tr_loss_list, val_f1_list, val_acc_list = [], [], [], []\n",
    "\n",
    "            for epoch_i in range(max_epochs):\n",
    "                model_pth = os.path.join(model_folder_pth,'bert_clf_{}.pt'.format(epoch_i))\n",
    "                checkpoint = torch.load(model_pth)\n",
    "                val_loss_list.append(checkpoint['val_loss'])\n",
    "                tr_loss_list.append(checkpoint['tr_loss'])\n",
    "                val_f1_list.append(checkpoint['val_f1'])\n",
    "                val_acc_list.append(checkpoint['val_acc'])\n",
    "\n",
    "\n",
    "            best_f1 = max(val_f1_list)\n",
    "            best_epoch = val_f1_list.index(best_f1)\n",
    "            best_f1_list.append((title, best_epoch, best_f1))\n",
    "\n",
    "\n",
    "            plt.figure(figsize=(15,4))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(tr_loss_list, label='tr loss', color='green')\n",
    "            plt.plot(val_loss_list, label='val loss', color='purple')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.plot(val_acc_list, label='val accuracy')\n",
    "            plt.plot(val_f1_list, label='val f1')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.suptitle(title)\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "for title, epoch, f1 in best_f1_list:\n",
    "    print('{} at epoch {}, F1: {:.4f}'.format(title, epoch, f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose and predict on test with best model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model is: BERT with lr=3e-06, warmup perc.=0.1, head=3_layers_M at epoch 6, F1: 0.8184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "final_model_pth = 'checkpoints_bert_subtaskA/16_4_2023-5.1/lr3e-06_warmup0.1_head_3_layers_M/bert_clf_6.pt'\n",
    "\n",
    "# Recover the best head configuration\n",
    "head_3_layers_M = nn.Sequential(\n",
    "                nn.Linear(768, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 2),\n",
    "                )\n",
    "# Initialize and recover model checkpoint\n",
    "final_model = BertClassifier(head=head_3_layers_M)\n",
    "final_model.load_state_dict(torch.load(final_model_pth).get('model_state_dict'))\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "# Prediction on test\n",
    "test_preds = test_clf(final_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(test_preds)\n",
    "# print occurrences of each class\n",
    "print(np.unique(test_preds, return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate result CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "id_list = [i for i in range(len(test_preds))]\n",
    "df = pd.DataFrame({'Id': id_list, 'Expected': test_preds})\n",
    "df.to_csv('final_bert_submission.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS FROM KAGGLE:**\n",
    "\n",
    "On the 30% available hidden test set it scores **0.83226**  F1 macro averaged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain best BERT classifier on whole development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "devel_dataset = ConcatDataset([tr_dataset, val_dataset])\n",
    "devel_dataloader = DataLoader(devel_dataset, sampler=RandomSampler(devel_dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "max_epochs = 12\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "# Num batches*num epochs\n",
    "tr_steps = len(devel_dataloader)*max_epochs\n",
    "\n",
    "# Best params from grid search\n",
    "lr = 3e-06\n",
    "warmup_perc = 0.1\n",
    "head_tuple = (nn.Sequential(\n",
    "                nn.Linear(768, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 2),\n",
    "                ), 'head_3_layers_M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "curr_date = datetime.now()\n",
    "\n",
    "retrain_model_folder_pth = './checkpoints_bert_retrain_subtaskA/{}_{}_{}-{}.{}/lr{}_warmup{}_{}'.format(curr_date.day, curr_date.month, curr_date.year, curr_date.hour, curr_date.minute,\n",
    "                                                                                            lr, warmup_perc, head_tuple[1])\n",
    "if not os.path.exists(retrain_model_folder_pth):\n",
    "    os.makedirs(retrain_model_folder_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize model\n",
    "bert_clf_model, loss_function, optimizer, scheduler = init_bert_clf(tr_steps=tr_steps, lr_rate=lr, scheduler_warmp_steps=warmup_perc*tr_steps, head=head_tuple[0])\n",
    "\n",
    "for epoch_i in range(max_epochs):\n",
    "    print('Epoch: {}'.format(epoch_i))\n",
    "\n",
    "    # Train\n",
    "    avg_epoch_loss_tr, acc_score_tr, f1_score_tr, bert_clf_model, optimizer, scheduler = train_clf(bert_clf_model, devel_dataloader, loss_function, optimizer, scheduler, device)\n",
    "    print('RETRAINING | Tr Loss: {} - Tr acc: {} - Tr f1: {}'.format(avg_epoch_loss_tr, acc_score_tr, f1_score_tr))\n",
    "\n",
    "    # Save\n",
    "    retrain_model_save_pth = os.path.join(retrain_model_folder_pth, 'bert_retrain_clf_{}.pt'.format(epoch_i))\n",
    "    torch.save({\n",
    "                'epoch': epoch_i,\n",
    "                'model_state_dict': bert_clf_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'schdeduler_state_dict': scheduler.state_dict(),\n",
    "                'tr_loss': avg_epoch_loss_tr,\n",
    "                'tr_acc': acc_score_tr,\n",
    "                'tr_f1': f1_score_tr,\n",
    "                }, retrain_model_save_pth)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize retrain loss, accuracy and f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tr_loss_list, tr_acc_list, tr_f1_list = [], [], []\n",
    "title = 'Retrain BERT with lr={}, warmup perc.={}, head={}'.format(lr, warmup_perc, head_tuple[1])\n",
    "\n",
    "\n",
    "for epoch_i in range(max_epochs):\n",
    "    model_pth = os.path.join(retrain_model_folder_pth,'bert_retrain_clf_{}.pt'.format(epoch_i))\n",
    "    checkpoint = torch.load(model_pth)\n",
    "    tr_loss_list.append(checkpoint['tr_loss'])\n",
    "    tr_acc_list.append(checkpoint['tr_acc'])\n",
    "    tr_f1_list.append(checkpoint['tr_f1'])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(tr_loss_list, label='retrain loss', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(tr_acc_list, label='retrain accuracy')\n",
    "plt.plot(tr_f1_list, label='retrain f1')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(title)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose and predict on test with best epoch checkpoint of retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "final_retrain_model_pth = 'checkpoints_bert_retrain_subtaskA/19_4_2023-14.27/lr3e-06_warmup0.1_head_3_layers_M/bert_retrain_clf_7.pt'\n",
    "\n",
    "# Recover the best head configuration\n",
    "head_3_layers_M = nn.Sequential(\n",
    "                nn.Linear(768, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(512, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 2),\n",
    "                )\n",
    "# Initialize and recover model checkpoint\n",
    "final_model_retrain = BertClassifier(head=head_3_layers_M)\n",
    "final_model_retrain.load_state_dict(torch.load(final_retrain_model_pth).get('model_state_dict'))\n",
    "\n",
    "final_model_retrain.to(device)\n",
    "\n",
    "# Prediction on test\n",
    "test_preds = test_clf(final_model_retrain, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(test_preds)\n",
    "# Print occurrences of each class\n",
    "print(np.unique(test_preds, return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate result CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "id_list = [i for i in range(len(test_preds))]\n",
    "df = pd.DataFrame({'Id': id_list, 'Expected': test_preds})\n",
    "df.to_csv('final_bert_retrain_submission.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check % of diff with final submission without retrain to estimate the range of the retrain test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n nlp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "bert_submission_retrain = test_preds\n",
    "\n",
    "bert_submission_no_retrain = pd.read_csv('final_bert_submission.csv')['Expected'].to_numpy()\n",
    "perc_diff = np.sum(bert_submission_no_retrain != bert_submission_retrain)/len(bert_submission_retrain)\n",
    "\n",
    "print('Percentual difference between retrain and no retrain test submission: {}%'.format(perc_diff*100))    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS FROM KAGGLE:**\n",
    "\n",
    "On the 30% available hidden test set it scores **0.77478**  F1 macro averaged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
