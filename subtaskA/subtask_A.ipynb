{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_csv('subtaskA_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡Se non ci fossero soldati non ci sarebbero gu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21/08/21]( [PRE-PRINT]\\n\\n📄__ \"Shedding of Inf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L'Aspirina non aumenta la sopravvivenza dei pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L'Italia non puo' dare armi lo vieta la Costit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  conspiratorial\n",
       "0  ⚡Se non ci fossero soldati non ci sarebbero gu...               0\n",
       "1  21/08/21]( [PRE-PRINT]\\n\\n📄__ \"Shedding of Inf...               1\n",
       "2  PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...               1\n",
       "3  L'Aspirina non aumenta la sopravvivenza dei pa...               0\n",
       "4  L'Italia non puo' dare armi lo vieta la Costit...               0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1845 entries, 0 to 1844\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    1842 non-null   object\n",
      " 1   conspiratorial  1845 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 29.0+ KB\n"
     ]
    }
   ],
   "source": [
    "texts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    comment_text  conspiratorial\n",
       "244          NaN               0\n",
       "263          NaN               0\n",
       "665          NaN               0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df[texts_df['comment_text'].isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete rows with NaN text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = texts_df[texts_df.comment_text.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1842 entries, 0 to 1844\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    1842 non-null   object\n",
      " 1   conspiratorial  1842 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.2+ KB\n"
     ]
    }
   ],
   "source": [
    "texts_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count positive and negatie samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    925\n",
       "0    917\n",
       "Name: conspiratorial, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.conspiratorial.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute '\\n' with ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.comment_text = texts_df.comment_text.apply(lambda text: text.replace('\\n\\n', ' ').replace('\\n', ' '))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting number of emojis for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "texts_df['emoji_count'] = texts_df.apply(lambda row: emoji.emoji_count(row.comment_text), result_type='expand', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "      <th>emoji_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡Se non ci fossero soldati non ci sarebbero gu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21/08/21]( [PRE-PRINT] 📄__ \"Shedding of Infect...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L'Aspirina non aumenta la sopravvivenza dei pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L'Italia non puo' dare armi lo vieta la Costit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  conspiratorial  \\\n",
       "0  ⚡Se non ci fossero soldati non ci sarebbero gu...               0   \n",
       "1  21/08/21]( [PRE-PRINT] 📄__ \"Shedding of Infect...               1   \n",
       "2  PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...               1   \n",
       "3  L'Aspirina non aumenta la sopravvivenza dei pa...               0   \n",
       "4  L'Italia non puo' dare armi lo vieta la Costit...               0   \n",
       "\n",
       "   emoji_count  \n",
       "0            1  \n",
       "1            6  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting ratio of full uppercase words for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppercase_ratio(text):\n",
    "    words = text.split()\n",
    "    num_upper_words = 0\n",
    "    for w in words:\n",
    "        num_upper_words += w.isupper()\n",
    "    return num_upper_words/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df['upper_case_ratio'] = texts_df.apply(lambda row: uppercase_ratio(row.comment_text), result_type='expand', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>upper_case_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡Se non ci fossero soldati non ci sarebbero gu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21/08/21]( [PRE-PRINT] 📄__ \"Shedding of Infect...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L'Aspirina non aumenta la sopravvivenza dei pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L'Italia non puo' dare armi lo vieta la Costit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ma non siete stufi di essere presi in giro??</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>**Ritengo questo audio piuttosto importante**:...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‼️ DIFFERENZA TRA SANGUE VACCINATO E NON VACCI...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.205128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Panzana pazzesca del leghista Siri: le misure ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tesla voleva portare a tutti energia gratuita ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  conspiratorial  \\\n",
       "0  ⚡Se non ci fossero soldati non ci sarebbero gu...               0   \n",
       "1  21/08/21]( [PRE-PRINT] 📄__ \"Shedding of Infect...               1   \n",
       "2  PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...               1   \n",
       "3  L'Aspirina non aumenta la sopravvivenza dei pa...               0   \n",
       "4  L'Italia non puo' dare armi lo vieta la Costit...               0   \n",
       "5       Ma non siete stufi di essere presi in giro??               1   \n",
       "6  **Ritengo questo audio piuttosto importante**:...               1   \n",
       "7  ‼️ DIFFERENZA TRA SANGUE VACCINATO E NON VACCI...               1   \n",
       "8  Panzana pazzesca del leghista Siri: le misure ...               1   \n",
       "9  Tesla voleva portare a tutti energia gratuita ...               0   \n",
       "\n",
       "   emoji_count  upper_case_ratio  \n",
       "0            1          0.000000  \n",
       "1            6          0.010870  \n",
       "2            0          0.040936  \n",
       "3            0          0.000000  \n",
       "4            0          0.000000  \n",
       "5            0          0.000000  \n",
       "6            3          0.000000  \n",
       "7            5          0.205128  \n",
       "8            0          0.000000  \n",
       "9            0          0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting ratio of bold words for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def bold_ratio(text):\n",
    "    num_words = len(text.split())\n",
    "\n",
    "\n",
    "    split_bolds = text.split('**')\n",
    "    split_bolds.pop()\n",
    "    count_bold = 0\n",
    "\n",
    "    for i, s in enumerate(split_bolds):\n",
    "        if(i%2 != 0):\n",
    "            count_bold += len(s.split())\n",
    "\n",
    "    return count_bold/num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df['bold_ratio'] = texts_df.apply(lambda row: bold_ratio(row.comment_text), result_type='expand', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>upper_case_ratio</th>\n",
       "      <th>bold_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>⚡Se non ci fossero soldati non ci sarebbero gu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21/08/21]( [PRE-PRINT] 📄__ \"Shedding of Infect...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.184783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L'Aspirina non aumenta la sopravvivenza dei pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L'Italia non puo' dare armi lo vieta la Costit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ma non siete stufi di essere presi in giro??</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>**Ritengo questo audio piuttosto importante**:...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‼️ DIFFERENZA TRA SANGUE VACCINATO E NON VACCI...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Panzana pazzesca del leghista Siri: le misure ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tesla voleva portare a tutti energia gratuita ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  conspiratorial  \\\n",
       "0  ⚡Se non ci fossero soldati non ci sarebbero gu...               0   \n",
       "1  21/08/21]( [PRE-PRINT] 📄__ \"Shedding of Infect...               1   \n",
       "2  PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...               1   \n",
       "3  L'Aspirina non aumenta la sopravvivenza dei pa...               0   \n",
       "4  L'Italia non puo' dare armi lo vieta la Costit...               0   \n",
       "5       Ma non siete stufi di essere presi in giro??               1   \n",
       "6  **Ritengo questo audio piuttosto importante**:...               1   \n",
       "7  ‼️ DIFFERENZA TRA SANGUE VACCINATO E NON VACCI...               1   \n",
       "8  Panzana pazzesca del leghista Siri: le misure ...               1   \n",
       "9  Tesla voleva portare a tutti energia gratuita ...               0   \n",
       "\n",
       "   emoji_count  upper_case_ratio  bold_ratio  \n",
       "0            1          0.000000    0.000000  \n",
       "1            6          0.010870    0.184783  \n",
       "2            0          0.040936    0.000000  \n",
       "3            0          0.000000    0.000000  \n",
       "4            0          0.000000    0.000000  \n",
       "5            0          0.000000    0.000000  \n",
       "6            3          0.000000    0.067568  \n",
       "7            5          0.205128    0.000000  \n",
       "8            0          0.000000    0.000000  \n",
       "9            0          0.000000    0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing datasets using stratified sampling\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, eval_index in split.split(texts_df, texts_df.conspiratorial):\n",
    "    train_df_full, eval_df_full = texts_df.iloc[train_index], texts_df.iloc[eval_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1473 entries, 1512 to 1771\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   comment_text      1473 non-null   object \n",
      " 1   conspiratorial    1473 non-null   int64  \n",
      " 2   emoji_count       1473 non-null   int64  \n",
      " 3   upper_case_ratio  1473 non-null   float64\n",
      " 4   bold_ratio        1473 non-null   float64\n",
      "dtypes: float64(2), int64(2), object(1)\n",
      "memory usage: 69.0+ KB\n",
      "None\n",
      "1    740\n",
      "0    733\n",
      "Name: conspiratorial, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df_full.info())\n",
    "print(train_df_full.conspiratorial.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 369 entries, 363 to 670\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   comment_text      369 non-null    object \n",
      " 1   conspiratorial    369 non-null    int64  \n",
      " 2   emoji_count       369 non-null    int64  \n",
      " 3   upper_case_ratio  369 non-null    float64\n",
      " 4   bold_ratio        369 non-null    float64\n",
      "dtypes: float64(2), int64(2), object(1)\n",
      "memory usage: 17.3+ KB\n",
      "None\n",
      "1    185\n",
      "0    184\n",
      "Name: conspiratorial, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(eval_df_full.info())\n",
    "print(eval_df_full.conspiratorial.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems balanced in term of positive and negative samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_bert = train_df_full[['comment_text', 'conspiratorial']]\n",
    "train_df_bert.columns = [\"text\", \"labels\"]\n",
    "eval_df_bert = eval_df_full[['comment_text', 'conspiratorial']]\n",
    "eval_df_bert.columns = [\"text\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 369 entries, 363 to 670\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    369 non-null    object\n",
      " 1   labels  369 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.6+ KB\n"
     ]
    }
   ],
   "source": [
    "eval_df_bert.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_labels = eval_df_bert.labels.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert-based models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_bert_clf(model_hgf_name, model_class, cased, eval_labels):\n",
    "    \n",
    "    batch_size = 8\n",
    "\n",
    "    # Num steps in epoch = num training samples / batch size\n",
    "    steps_per_epoch = int(np.ceil(len(train_df_bert) / float(batch_size)))\n",
    "\n",
    "    print('Each epoch will have {:,} steps.'.format(steps_per_epoch))\n",
    "\n",
    "\n",
    "    # Optional model configuration\n",
    "    model_args = ClassificationArgs(num_train_epochs=20, do_lower_case=cased, evaluate_during_training=True, evaluate_during_training_verbose=True, # Main options\n",
    "                                    use_multiprocessing=False, use_multiprocessing_for_evaluation=False, overwrite_output_dir=True,  # System configurations\n",
    "                                    output_dir='out_'+model_hgf_name,\n",
    "                                    eval_batch_size=batch_size, train_batch_size=batch_size, evaluate_during_training_steps=steps_per_epoch, # Batch sizes and steps\n",
    "                                    use_early_stopping=True, early_stopping_metric='eval_loss', early_stopping_patience=2, early_stopping_metric_minimize=True, # Early stopping\n",
    "                                    early_stopping_delta=0.01, early_stopping_consider_epochs=True\n",
    "                                    )\n",
    "\n",
    "    # Create a ClassificationModel\n",
    "    model = ClassificationModel(model_class, model_hgf_name, args=model_args, use_cuda=cuda_available)\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(train_df_bert, eval_df=eval_df_bert)\n",
    "\n",
    "    # Predict on train\n",
    "    full_pred_tr = model.predict(train_df_bert.text.tolist())\n",
    "    pred_tr = full_pred_tr[0]\n",
    "    raw_pred_tr = full_pred_tr[1]\n",
    "\n",
    "    # Predict on evaluation\n",
    "    full_pred_eval = model.predict(eval_df_bert.text.tolist())\n",
    "    pred_eval = full_pred_eval[0]\n",
    "    raw_pred_eval = full_pred_eval[1]\n",
    "\n",
    "    # Make classification report\n",
    "    clf_report = classification_report(eval_labels, pred_eval, target_names=['non-conspiratorial', 'conspiratorial'], digits=4)\n",
    "\n",
    "    return model, clf_report, raw_pred_tr, raw_pred_eval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate various BERT based models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class, huggingface name, cased\n",
    "\n",
    "bert_models_list = [\n",
    "    (\"bert\", \"dbmdz/bert-base-italian-cased\", True),\n",
    "    (\"distilbert\", \"indigo-ai/BERTino\", False),\n",
    "    (\"bert\", \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\", False),\n",
    "    (\"bert\", \"dbmdz/bert-base-italian-xxl-cased\", True)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each epoch will have 185 steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_2_2\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.46525690381548174, 'tp': 144, 'tn': 126, 'fp': 58, 'fn': 41, 'auroc': 0.826659811985899, 'auprc': 0.8329765558577391, 'eval_loss': 0.5280092117634225}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 0/20. Running Loss:    0.2527: 100%|██████████| 185/185 [00:18<00:00,  9.89it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.46525690381548174, 'tp': 144, 'tn': 126, 'fp': 58, 'fn': 41, 'auroc': 0.826659811985899, 'auprc': 0.8329765558577391, 'eval_loss': 0.5280092117634225}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 2 of 20:   5%|▌         | 1/20 [00:20<06:33, 20.72s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.4890306089515059, 'tp': 110, 'tn': 161, 'fp': 23, 'fn': 75, 'auroc': 0.8498531139835488, 'auprc': 0.8553317301176337, 'eval_loss': 0.5755807186694856}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 2 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "Epoch 2 of 20:   5%|▌         | 1/20 [00:36<11:31, 36.40s/it]\n",
      "Epochs 1/20. Running Loss:    0.0235:  99%|█████████▉| 184/185 [00:15<00:00, 11.73it/s]\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to out_dbmdz/bert-base-italian-cased.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|██████████| 185/185 [00:02<00:00, 75.94it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|██████████| 47/47 [00:00<00:00, 77.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "---- dbmdz/bert-base-italian-cased ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.6822    0.8750    0.7667       184\n",
      "    conspiratorial     0.8271    0.5946    0.6918       185\n",
      "\n",
      "          accuracy                         0.7344       369\n",
      "         macro avg     0.7546    0.7348    0.7292       369\n",
      "      weighted avg     0.7548    0.7344    0.7291       369\n",
      "\n",
      "Each epoch will have 185 steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indigo-ai/BERTino were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at indigo-ai/BERTino and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_distilbert_128_2_2\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_distilbert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.49872199618098345, 'tp': 129, 'tn': 147, 'fp': 37, 'fn': 56, 'auroc': 0.8364424206815511, 'auprc': 0.8256160226948945, 'eval_loss': 0.5115755771068816}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 0/20. Running Loss:    0.3357: 100%|██████████| 185/185 [00:09<00:00, 18.99it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_distilbert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.49872199618098345, 'tp': 129, 'tn': 147, 'fp': 37, 'fn': 56, 'auroc': 0.8364424206815511, 'auprc': 0.8256160226948945, 'eval_loss': 0.5115755771068816}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 2 of 20:   5%|▌         | 1/20 [00:10<03:28, 10.95s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_distilbert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5533039921343454, 'tp': 127, 'tn': 158, 'fp': 26, 'fn': 58, 'auroc': 0.8744418331374854, 'auprc': 0.8695141409306358, 'eval_loss': 0.5103171328280834}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 2 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "Epoch 2 of 20:   5%|▌         | 1/20 [00:19<06:17, 19.85s/it]\n",
      "Epochs 1/20. Running Loss:    0.1060:  99%|█████████▉| 184/185 [00:08<00:00, 20.67it/s]\n",
      "INFO:simpletransformers.classification.classification_model: Training of distilbert model complete. Saved to out_indigo-ai/BERTino.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "Running Prediction: 100%|██████████| 185/185 [00:58<00:00,  3.19it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "Running Prediction: 100%|██████████| 47/47 [00:03<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "---- indigo-ai/BERTino ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.7315    0.8587    0.7900       184\n",
      "    conspiratorial     0.8301    0.6865    0.7515       185\n",
      "\n",
      "          accuracy                         0.7724       369\n",
      "         macro avg     0.7808    0.7726    0.7707       369\n",
      "      weighted avg     0.7809    0.7724    0.7707       369\n",
      "\n",
      "Each epoch will have 185 steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_2_2\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.49615047690243447, 'tp': 136, 'tn': 140, 'fp': 44, 'fn': 49, 'auroc': 0.831551116333725, 'auprc': 0.8230867206739434, 'eval_loss': 0.50248957694845}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 0/20. Running Loss:    0.1108: 100%|██████████| 185/185 [00:23<00:00,  7.99it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.49615047690243447, 'tp': 136, 'tn': 140, 'fp': 44, 'fn': 49, 'auroc': 0.831551116333725, 'auprc': 0.8230867206739434, 'eval_loss': 0.50248957694845}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 2 of 20:   5%|▌         | 1/20 [00:26<08:18, 26.23s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.4541997762895656, 'tp': 88, 'tn': 171, 'fp': 13, 'fn': 97, 'auroc': 0.8438454759106933, 'auprc': 0.8366183778314862, 'eval_loss': 0.6066999638334234}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 2 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "Epoch 2 of 20:   5%|▌         | 1/20 [00:47<15:10, 47.90s/it]\n",
      "Epochs 1/20. Running Loss:    0.0395:  99%|█████████▉| 184/185 [00:21<00:00,  8.49it/s]\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to out_m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|██████████| 185/185 [00:02<00:00, 75.35it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|██████████| 47/47 [00:00<00:00, 77.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "---- m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.6381    0.9293    0.7566       184\n",
      "    conspiratorial     0.8713    0.4757    0.6154       185\n",
      "\n",
      "          accuracy                         0.7019       369\n",
      "         macro avg     0.7547    0.7025    0.6860       369\n",
      "      weighted avg     0.7550    0.7019    0.6858       369\n",
      "\n",
      "Each epoch will have 185 steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-xxl-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_2_2\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.49940042519993494, 'tp': 168, 'tn': 103, 'fp': 81, 'fn': 17, 'auroc': 0.8576233842538191, 'auprc': 0.8655766210209295, 'eval_loss': 0.5199006263245928}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 0/20. Running Loss:    0.4360: 100%|██████████| 185/185 [00:18<00:00,  9.88it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.49940042519993494, 'tp': 168, 'tn': 103, 'fp': 81, 'fn': 17, 'auroc': 0.8576233842538191, 'auprc': 0.8655766210209295, 'eval_loss': 0.5199006263245928}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 2 of 20:   5%|▌         | 1/20 [00:21<06:42, 21.20s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5466295056486724, 'tp': 168, 'tn': 113, 'fp': 71, 'fn': 17, 'auroc': 0.8846210340775559, 'auprc': 0.8853384137861449, 'eval_loss': 0.5059794162182097}\n",
      "Epochs 1/20. Running Loss:    0.0320: 100%|██████████| 185/185 [00:19<00:00,  9.36it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5466295056486724, 'tp': 168, 'tn': 113, 'fp': 71, 'fn': 17, 'auroc': 0.8846210340775559, 'auprc': 0.8853384137861449, 'eval_loss': 0.5059794162182097}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 3 of 20:  10%|█         | 2/20 [00:43<06:30, 21.68s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.6159572914375857, 'tp': 145, 'tn': 153, 'fp': 31, 'fn': 40, 'auroc': 0.8852085781433607, 'auprc': 0.8909017515757448, 'eval_loss': 0.6698458904915667}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 2/20. Running Loss:    0.0028: 100%|██████████| 185/185 [00:18<00:00, 10.23it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.6159572914375857, 'tp': 145, 'tn': 153, 'fp': 31, 'fn': 40, 'auroc': 0.8852085781433607, 'auprc': 0.8909017515757448, 'eval_loss': 0.6698458904915667}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 2 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "Epoch 3 of 20:  10%|█         | 2/20 [01:03<09:33, 31.84s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to out_dbmdz/bert-base-italian-xxl-cased.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|██████████| 185/185 [00:02<00:00, 76.00it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|██████████| 47/47 [00:00<00:00, 77.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "---- dbmdz/bert-base-italian-xxl-cased ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.7927    0.8315    0.8117       184\n",
      "    conspiratorial     0.8239    0.7838    0.8033       185\n",
      "\n",
      "          accuracy                         0.8076       369\n",
      "         macro avg     0.8083    0.8077    0.8075       369\n",
      "      weighted avg     0.8083    0.8076    0.8075       369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_bert_models_list = []\n",
    "eval_report_list = []\n",
    "raw_pred_list_tr = []\n",
    "raw_pred_list_val = []\n",
    "\n",
    "for model_class, model_hgf_name, cased in bert_models_list:\n",
    "\n",
    "    model, clf_report, raw_pred_tr, raw_pred_val = train_validate_bert_clf(model_hgf_name, model_class, cased, eval_labels)\n",
    "    trained_bert_models_list.append(model)\n",
    "    eval_report_list.append(clf_report)\n",
    "    raw_pred_list_tr.append(raw_pred_tr)\n",
    "    raw_pred_list_val.append(raw_pred_val)\n",
    "\n",
    "    # Print model stats\n",
    "    print('#################################')\n",
    "    print('----', model_hgf_name, '----')\n",
    "    print(clf_report)\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---- dbmdz/bert-base-italian-cased ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.6822    0.8750    0.7667       184\n",
      "    conspiratorial     0.8271    0.5946    0.6918       185\n",
      "\n",
      "          accuracy                         0.7344       369\n",
      "         macro avg     0.7546    0.7348    0.7292       369\n",
      "      weighted avg     0.7548    0.7344    0.7291       369\n",
      "\n",
      "\n",
      "\n",
      "---- indigo-ai/BERTino ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.7315    0.8587    0.7900       184\n",
      "    conspiratorial     0.8301    0.6865    0.7515       185\n",
      "\n",
      "          accuracy                         0.7724       369\n",
      "         macro avg     0.7808    0.7726    0.7707       369\n",
      "      weighted avg     0.7809    0.7724    0.7707       369\n",
      "\n",
      "\n",
      "\n",
      "---- m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.6381    0.9293    0.7566       184\n",
      "    conspiratorial     0.8713    0.4757    0.6154       185\n",
      "\n",
      "          accuracy                         0.7019       369\n",
      "         macro avg     0.7547    0.7025    0.6860       369\n",
      "      weighted avg     0.7550    0.7019    0.6858       369\n",
      "\n",
      "\n",
      "\n",
      "---- dbmdz/bert-base-italian-xxl-cased ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.7927    0.8315    0.8117       184\n",
      "    conspiratorial     0.8239    0.7838    0.8033       185\n",
      "\n",
      "          accuracy                         0.8076       369\n",
      "         macro avg     0.8083    0.8077    0.8075       369\n",
      "      weighted avg     0.8083    0.8076    0.8075       369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print model stats\n",
    "\n",
    "for i, clf_report in enumerate(eval_report_list):\n",
    "\n",
    "    model_hgf_name = bert_models_list[i][1]\n",
    "    print('\\n')\n",
    "    print('----', model_hgf_name, '----')\n",
    "    print(clf_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification over Bert SoftMax + extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize extracted features and convert to numpy 2d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1473, 3)\n",
      "(369, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "extracted_cols = ['emoji_count', 'upper_case_ratio', 'bold_ratio']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "extracted_features_tr = scaler.fit_transform(train_df_full[extracted_cols].to_numpy())\n",
    "extracted_features_val = scaler.fit_transform(eval_df_full[extracted_cols].to_numpy())\n",
    "\n",
    "\n",
    "print(extracted_features_tr.shape)\n",
    "print(extracted_features_val.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine extracted features with raw BERT predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1473, 5)\n",
      "(369, 5)\n"
     ]
    }
   ],
   "source": [
    "raw_pred_tr = raw_pred_list_tr[3]\n",
    "raw_pred_val = raw_pred_list_val[3]\n",
    "\n",
    "pred_with_extrac_tr = np.concatenate((raw_pred_tr, extracted_features_tr), axis=1)\n",
    "pred_with_extrac_val = np.concatenate((raw_pred_val, extracted_features_val), axis=1)\n",
    "\n",
    "print(pred_with_extrac_tr.shape)\n",
    "print(pred_with_extrac_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that builds a NN from a list configuration containing layer neurons\n",
    "def build_nn(layers_config, input, droput=0.0):\n",
    "    model = Sequential()\n",
    "\n",
    "    for i, layer_neurons in enumerate(layers_config):\n",
    "        # Add hidden layers with the current number of neurons\n",
    "        if i==0: # First layer\n",
    "            model.add(Dense(layer_neurons, activation='relu', input_dim=input.shape[1]))\n",
    "        else:\n",
    "            model.add(Dropout(droput))\n",
    "            model.add(Dense(layer_neurons, activation='relu', input_dim=layers_config[i-1]))\n",
    "    \n",
    "    # Add the output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with  [8] configuration - Validation Score: 0.8021680116653442 - Epochs: 6\n",
      "NN with  [12] configuration - Validation Score: 0.8048780560493469 - Epochs: 2\n",
      "NN with  [16] configuration - Validation Score: 0.7859078645706177 - Epochs: 5\n",
      "NN with  [24] configuration - Validation Score: 0.8048780560493469 - Epochs: 4\n"
     ]
    }
   ],
   "source": [
    "# Define a list of possible lists of numbers of neurons for each layer\n",
    "layers_config_list = [[8], [12], [16], [24]]\n",
    "\n",
    "history_list = []\n",
    "models_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# Iterate over all possible combinations of number of layers and neurons\n",
    "for layers_config in layers_config_list:\n",
    "    # Builds NN architecture\n",
    "    model = build_nn(layers_config, input=pred_with_extrac_tr, droput=0.3)\n",
    "\n",
    "    # Add early stopping that checks val loss\n",
    "    earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\",  mode =\"min\", patience = 2, restore_best_weights = True)\n",
    "    \n",
    "    # Train the model on the training set\n",
    "    history = model.fit(pred_with_extrac_tr, train_df_bert.labels, validation_data = (pred_with_extrac_val,eval_df_bert.labels),\n",
    "        epochs=50, batch_size=32, verbose=0, callbacks=[earlystopping])\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    _, accuracy = model.evaluate(pred_with_extrac_val, eval_df_bert.labels, verbose=0)\n",
    "\n",
    "    # Save and show results\n",
    "    print('NN with ', layers_config, 'configuration - Validation Score:', accuracy,\n",
    "        '- Epochs:', earlystopping.stopped_epoch)\n",
    "    history_list.append(history)\n",
    "    models_list.append(model)\n",
    "    val_acc_list.append(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b871520dad97a5289b2adf41ad6a240d97c9b001919f4a48e8f233f7be13d0e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
