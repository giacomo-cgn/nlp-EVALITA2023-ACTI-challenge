{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_csv('subtaskA_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>âš¡Se non ci fossero soldati non ci sarebbero gu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21/08/21]( [PRE-PRINT]\\n\\nðŸ“„__ \"Shedding of Inf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L'Aspirina non aumenta la sopravvivenza dei pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L'Italia non puo' dare armi lo vieta la Costit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  conspiratorial\n",
       "0  âš¡Se non ci fossero soldati non ci sarebbero gu...               0\n",
       "1  21/08/21]( [PRE-PRINT]\\n\\nðŸ“„__ \"Shedding of Inf...               1\n",
       "2  PAURA E DELIRIO ALLA CNN: IL MINISTERO DELLA V...               1\n",
       "3  L'Aspirina non aumenta la sopravvivenza dei pa...               0\n",
       "4  L'Italia non puo' dare armi lo vieta la Costit...               0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1845 entries, 0 to 1844\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    1842 non-null   object\n",
      " 1   conspiratorial  1845 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 29.0+ KB\n"
     ]
    }
   ],
   "source": [
    "texts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>conspiratorial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    comment_text  conspiratorial\n",
       "244          NaN               0\n",
       "263          NaN               0\n",
       "665          NaN               0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df[texts_df['comment_text'].isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete rows with NaN text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = texts_df[texts_df.comment_text.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1842 entries, 0 to 1844\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    1842 non-null   object\n",
      " 1   conspiratorial  1842 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.2+ KB\n"
     ]
    }
   ],
   "source": [
    "texts_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count positive and negatie samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    925\n",
       "0    917\n",
       "Name: conspiratorial, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.conspiratorial.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute '\\n' with ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.comment_text = texts_df.comment_text.apply(lambda text: text.replace('\\n', ' '))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing datasets using stratified sampling\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, eval_index in split.split(texts_df, texts_df.conspiratorial):\n",
    "    train_df, eval_df = texts_df.iloc[train_index], texts_df.iloc[eval_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1473 entries, 1512 to 1771\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    1473 non-null   object\n",
      " 1   conspiratorial  1473 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 34.5+ KB\n",
      "None\n",
      "1    740\n",
      "0    733\n",
      "Name: conspiratorial, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())\n",
    "print(train_df.conspiratorial.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 369 entries, 363 to 670\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   comment_text    369 non-null    object\n",
      " 1   conspiratorial  369 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.6+ KB\n",
      "None\n",
      "1    185\n",
      "0    184\n",
      "Name: conspiratorial, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(eval_df.info())\n",
    "print(eval_df.conspiratorial.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems balanced in term of positive and negative samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = [\"text\", \"labels\"]\n",
    "eval_df.columns = [\"text\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(cuda_available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_labels = eval_df.labels.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert-based models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_bert_clf(model_hgf_name, model_class, cased, eval_labels):\n",
    "    \n",
    "    batch_size = 8\n",
    "\n",
    "    # Num steps in epoch = num training samples / batch size\n",
    "    steps_per_epoch = int(np.ceil(len(train_df) / float(batch_size)))\n",
    "\n",
    "    print('Each epoch will have {:,} steps.'.format(steps_per_epoch))\n",
    "\n",
    "\n",
    "    # Optional model configuration\n",
    "    model_args = ClassificationArgs(num_train_epochs=20, do_lower_case=cased, evaluate_during_training=True, evaluate_during_training_verbose=True, # Main options\n",
    "                                    use_multiprocessing=False, use_multiprocessing_for_evaluation=False, overwrite_output_dir=True,  # System configurations\n",
    "                                    output_dir='out_'+model_hgf_name,\n",
    "                                    eval_batch_size=batch_size, train_batch_size=batch_size, evaluate_during_training_steps=steps_per_epoch, # Batch sizes and steps\n",
    "                                    use_early_stopping=True, early_stopping_metric='eval_loss', early_stopping_patience=2, early_stopping_metric_minimize=True, # Early stopping\n",
    "                                    early_stopping_delta=0.01, early_stopping_consider_epochs=True\n",
    "                                    )\n",
    "\n",
    "    # Create a ClassificationModel\n",
    "    model = ClassificationModel(model_class, model_hgf_name, args=model_args, use_cuda=cuda_available)\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(train_df, eval_df=eval_df)\n",
    "\n",
    "    # Predict on evaluation\n",
    "    full_pred = model.predict(eval_df.text.tolist())\n",
    "    pred = full_pred[0]\n",
    "    raw_pred = full_pred[1]\n",
    "\n",
    "    # Make classification report\n",
    "    clf_report = classification_report(eval_labels, pred, target_names=['non-conspiratorial', 'conspiratorial'], digits=4)\n",
    "\n",
    "    return model, clf_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate various BERT based models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class, huggingface name, cased\n",
    "\n",
    "bert_models_list = [\n",
    "    (\"bert\", \"dbmdz/bert-base-italian-cased\", True),\n",
    "    (\"distilbert\", \"indigo-ai/BERTino\", False),\n",
    "    (\"bert\", \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\", False),\n",
    "    (\"bert\", \"dbmdz/bert-base-italian-xxl-cased\", True)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each epoch will have 185 steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_2_2\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.45272788568347333, 'tp': 154, 'tn': 112, 'fp': 72, 'fn': 31, 'auroc': 0.8223413631022327, 'auprc': 0.8306991338381952, 'eval_loss': 0.5532527680092669}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 0/20. Running Loss:    0.7949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:24<00:00,  7.64it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.45272788568347333, 'tp': 154, 'tn': 112, 'fp': 72, 'fn': 31, 'auroc': 0.8223413631022327, 'auprc': 0.8306991338381952, 'eval_loss': 0.5532527680092669}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 2 of 20:   5%|â–Œ         | 1/20 [00:28<09:05, 28.69s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5848364342829518, 'tp': 161, 'tn': 130, 'fp': 54, 'fn': 24, 'auroc': 0.8629847238542889, 'auprc': 0.8513361071067832, 'eval_loss': 0.4761002723206865}\n",
      "Epochs 1/20. Running Loss:    0.1621: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:17<00:00, 10.82it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5848364342829518, 'tp': 161, 'tn': 130, 'fp': 54, 'fn': 24, 'auroc': 0.8629847238542889, 'auprc': 0.8513361071067832, 'eval_loss': 0.4761002723206865}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 3 of 20:  10%|â–ˆ         | 2/20 [00:47<06:54, 23.01s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5501336704392294, 'tp': 144, 'tn': 142, 'fp': 42, 'fn': 41, 'auroc': 0.855023501762632, 'auprc': 0.8479390036754573, 'eval_loss': 0.7143246620259387}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 2/20. Running Loss:    0.0053: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:16<00:00, 11.25it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5501336704392294, 'tp': 144, 'tn': 142, 'fp': 42, 'fn': 41, 'auroc': 0.855023501762632, 'auprc': 0.8479390036754573, 'eval_loss': 0.7143246620259387}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 2 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "Epoch 3 of 20:  10%|â–ˆ         | 2/20 [01:06<09:55, 33.06s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to out_dbmdz/bert-base-italian-cased.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 77.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "---- dbmdz/bert-base-italian-cased ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.7760    0.7717    0.7738       184\n",
      "    conspiratorial     0.7742    0.7784    0.7763       185\n",
      "\n",
      "          accuracy                         0.7751       369\n",
      "         macro avg     0.7751    0.7751    0.7751       369\n",
      "      weighted avg     0.7751    0.7751    0.7751       369\n",
      "\n",
      "Each epoch will have 185 steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indigo-ai/BERTino were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at indigo-ai/BERTino and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_distilbert_128_2_2\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_distilbert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5293097353260422, 'tp': 160, 'tn': 120, 'fp': 64, 'fn': 25, 'auroc': 0.859297884841363, 'auprc': 0.8631299529337918, 'eval_loss': 0.5198643461186835}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 0/20. Running Loss:    0.1436: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:09<00:00, 18.82it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_distilbert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5293097353260422, 'tp': 160, 'tn': 120, 'fp': 64, 'fn': 25, 'auroc': 0.859297884841363, 'auprc': 0.8631299529337918, 'eval_loss': 0.5198643461186835}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 2 of 20:   5%|â–Œ         | 1/20 [00:11<03:29, 11.03s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_distilbert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5558071549004974, 'tp': 147, 'tn': 140, 'fp': 44, 'fn': 38, 'auroc': 0.870475910693302, 'auprc': 0.8761613923962441, 'eval_loss': 0.4658356524528341}\n",
      "Epochs 1/20. Running Loss:    0.1039: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:09<00:00, 18.70it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_distilbert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5558071549004974, 'tp': 147, 'tn': 140, 'fp': 44, 'fn': 38, 'auroc': 0.870475910693302, 'auprc': 0.8761613923962441, 'eval_loss': 0.4658356524528341}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 3 of 20:  10%|â–ˆ         | 2/20 [00:22<03:22, 11.24s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_distilbert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5341133913973999, 'tp': 145, 'tn': 138, 'fp': 46, 'fn': 40, 'auroc': 0.8460487661574618, 'auprc': 0.8016924510065926, 'eval_loss': 0.8513419070142381}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 2/20. Running Loss:    0.4060: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:11<00:00, 16.51it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_distilbert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5341133913973999, 'tp': 145, 'tn': 138, 'fp': 46, 'fn': 40, 'auroc': 0.8460487661574618, 'auprc': 0.8016924510065926, 'eval_loss': 0.8513419070142381}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 2 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "Epoch 3 of 20:  10%|â–ˆ         | 2/20 [00:37<05:35, 18.62s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of distilbert model complete. Saved to out_indigo-ai/BERTino.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "Running Prediction: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:03<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "---- indigo-ai/BERTino ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.7753    0.7500    0.7624       184\n",
      "    conspiratorial     0.7592    0.7838    0.7713       185\n",
      "\n",
      "          accuracy                         0.7669       369\n",
      "         macro avg     0.7672    0.7669    0.7669       369\n",
      "      weighted avg     0.7672    0.7669    0.7669       369\n",
      "\n",
      "Each epoch will have 185 steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_2_2\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.4396014886453105, 'tp': 159, 'tn': 103, 'fp': 81, 'fn': 26, 'auroc': 0.8188454759106933, 'auprc': 0.8130329945016179, 'eval_loss': 0.5378863557856134}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 0/20. Running Loss:    0.6772: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:26<00:00,  7.11it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.4396014886453105, 'tp': 159, 'tn': 103, 'fp': 81, 'fn': 26, 'auroc': 0.8188454759106933, 'auprc': 0.8130329945016179, 'eval_loss': 0.5378863557856134}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 2 of 20:   5%|â–Œ         | 1/20 [00:32<10:10, 32.13s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.40246432975864, 'tp': 182, 'tn': 58, 'fp': 126, 'fn': 3, 'auroc': 0.8483989424206817, 'auprc': 0.8477648242596987, 'eval_loss': 0.7117580251490816}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 2 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "Epoch 2 of 20:   5%|â–Œ         | 1/20 [00:55<17:29, 55.21s/it]\n",
      "Epochs 1/20. Running Loss:    0.0620:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 184/185 [00:23<00:00,  7.97it/s]\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to out_m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 73.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "---- m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.9508    0.3152    0.4735       184\n",
      "    conspiratorial     0.5909    0.9838    0.7383       185\n",
      "\n",
      "          accuracy                         0.6504       369\n",
      "         macro avg     0.7709    0.6495    0.6059       369\n",
      "      weighted avg     0.7704    0.6504    0.6063       369\n",
      "\n",
      "Each epoch will have 185 steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-xxl-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_2_2\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]/home/giacomo/mambaforge/envs/nlp/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.4226490291789365, 'tp': 67, 'tn': 179, 'fp': 5, 'fn': 118, 'auroc': 0.8290687426556992, 'auprc': 0.8416414494371032, 'eval_loss': 0.7436292120750915}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 0/20. Running Loss:    0.9131: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:19<00:00,  9.46it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.4226490291789365, 'tp': 67, 'tn': 179, 'fp': 5, 'fn': 118, 'auroc': 0.8290687426556992, 'auprc': 0.8416414494371032, 'eval_loss': 0.7436292120750915}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 2 of 20:   5%|â–Œ         | 1/20 [00:21<06:55, 21.86s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.6317353271613385, 'tp': 154, 'tn': 147, 'fp': 37, 'fn': 31, 'auroc': 0.89463866039953, 'auprc': 0.9016095469698087, 'eval_loss': 0.4334098734754197}\n",
      "Epochs 1/20. Running Loss:    0.0185: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:19<00:00,  9.46it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.6317353271613385, 'tp': 154, 'tn': 147, 'fp': 37, 'fn': 31, 'auroc': 0.89463866039953, 'auprc': 0.9016095469698087, 'eval_loss': 0.4334098734754197}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 3 of 20:  10%|â–ˆ         | 2/20 [00:43<06:33, 21.85s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5457879066800394, 'tp': 149, 'tn': 136, 'fp': 48, 'fn': 36, 'auroc': 0.8781286721504112, 'auprc': 0.8850884645231065, 'eval_loss': 1.0709558915584645}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 2/20. Running Loss:    0.0035: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:19<00:00,  9.40it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5457879066800394, 'tp': 149, 'tn': 136, 'fp': 48, 'fn': 36, 'auroc': 0.8781286721504112, 'auprc': 0.8850884645231065, 'eval_loss': 1.0709558915584645}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 2 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "Epoch 3 of 20:  10%|â–ˆ         | 2/20 [01:07<10:11, 33.95s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to out_dbmdz/bert-base-italian-xxl-cased.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 74.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "---- dbmdz/bert-base-italian-xxl-cased ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.7907    0.7391    0.7640       184\n",
      "    conspiratorial     0.7563    0.8054    0.7801       185\n",
      "\n",
      "          accuracy                         0.7724       369\n",
      "         macro avg     0.7735    0.7723    0.7721       369\n",
      "      weighted avg     0.7735    0.7724    0.7721       369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_bert_models_list = []\n",
    "eval_report_list = []\n",
    "\n",
    "for model_class, model_hgf_name, cased in bert_models_list:\n",
    "\n",
    "    model, clf_report = train_validate_bert_clf(model_hgf_name, model_class, cased, eval_labels)\n",
    "    trained_bert_models_list.append(model)\n",
    "    eval_report_list.append(clf_report)\n",
    "\n",
    "    # Print model stats\n",
    "    print('#################################')\n",
    "    print('----', model_hgf_name, '----')\n",
    "    print(clf_report)\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---- dbmdz/bert-base-italian-cased ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.6929    0.9076    0.7859       184\n",
      "    conspiratorial     0.8672    0.6000    0.7093       185\n",
      "\n",
      "          accuracy                         0.7534       369\n",
      "         macro avg     0.7801    0.7538    0.7476       369\n",
      "      weighted avg     0.7803    0.7534    0.7475       369\n",
      "\n",
      "\n",
      "\n",
      "---- indigo-ai/BERTino ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.7626    0.8207    0.7906       184\n",
      "    conspiratorial     0.8070    0.7459    0.7753       185\n",
      "\n",
      "          accuracy                         0.7832       369\n",
      "         macro avg     0.7848    0.7833    0.7829       369\n",
      "      weighted avg     0.7849    0.7832    0.7829       369\n",
      "\n",
      "\n",
      "\n",
      "---- m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.7486    0.7446    0.7466       184\n",
      "    conspiratorial     0.7473    0.7514    0.7493       185\n",
      "\n",
      "          accuracy                         0.7480       369\n",
      "         macro avg     0.7480    0.7480    0.7480       369\n",
      "      weighted avg     0.7480    0.7480    0.7480       369\n",
      "\n",
      "\n",
      "\n",
      "---- dbmdz/bert-base-italian-xxl-cased ----\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial     0.8864    0.6359    0.7405       184\n",
      "    conspiratorial     0.7173    0.9189    0.8057       185\n",
      "\n",
      "          accuracy                         0.7778       369\n",
      "         macro avg     0.8018    0.7774    0.7731       369\n",
      "      weighted avg     0.8016    0.7778    0.7732       369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print model stats\n",
    "\n",
    "for i, clf_report in enumerate(eval_report_list):\n",
    "\n",
    "    model_hgf_name = bert_models_list[i][1]\n",
    "    print('\\n')\n",
    "    print('----', model_hgf_name, '----')\n",
    "    print(clf_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other models\n",
    "\n",
    "# model = ClassificationModel(\"bert\", \"dbmdz/bert-base-italian-cased\", args=model_args, use_cuda=cuda_available)\n",
    "# model = ClassificationModel(\"distilbert\", \"indigo-ai/BERTino\", args=model_args, use_cuda=cuda_available) --- 0.81 with 5 epochs\n",
    "# model = ClassificationModel(\"bert\", \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\", args=model_args, use_cuda=cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each epoch will have 185 steps.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# Num steps in epoch = num training samples / batch size\n",
    "steps_per_epoch = int(np.ceil(len(train_df) / float(batch_size)))\n",
    "\n",
    "print('Each epoch will have {:,} steps.'.format(steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-xxl-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args = ClassificationArgs(num_train_epochs=20, do_lower_case=False, evaluate_during_training=True, evaluate_during_training_verbose=True, # Main options\n",
    "                                use_multiprocessing=False, use_multiprocessing_for_evaluation=False, overwrite_output_dir=True, # System configurations\n",
    "                                eval_batch_size=batch_size, train_batch_size=batch_size, evaluate_during_training_steps=steps_per_epoch, # Batch sizes and steps\n",
    "                                use_early_stopping=True, early_stopping_metric='eval_loss', early_stopping_patience=2, early_stopping_metric_minimize=True, # Early stopping\n",
    "                                early_stopping_delta=0.001, early_stopping_consider_epochs=True\n",
    "                                   )\n",
    "\n",
    "# Create a ClassificationModel\n",
    "\n",
    "model = ClassificationModel(\"bert\", \"dbmdz/bert-base-italian-xxl-cased\", args=model_args, use_cuda=cuda_available)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_2_2\n",
      "Epoch 1 of 20:   0%|          | 0/20 [00:00<?, ?it/s]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.32644187856724644, 'tp': 51, 'tn': 177, 'fp': 7, 'fn': 134, 'auroc': 0.8220329024676851, 'auprc': 0.813226434485485, 'eval_loss': 0.7783598798386594}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 0/20. Running Loss:    0.3132: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:17<00:00, 10.41it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.32644187856724644, 'tp': 51, 'tn': 177, 'fp': 7, 'fn': 134, 'auroc': 0.8220329024676851, 'auprc': 0.813226434485485, 'eval_loss': 0.7783598798386594}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 2 of 20:   5%|â–Œ         | 1/20 [00:19<06:08, 19.41s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.590999620240559, 'tp': 138, 'tn': 155, 'fp': 29, 'fn': 47, 'auroc': 0.8816392479435958, 'auprc': 0.8700721752818596, 'eval_loss': 0.4868150771932399}\n",
      "Epochs 1/20. Running Loss:    0.0566: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:18<00:00, 10.01it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.590999620240559, 'tp': 138, 'tn': 155, 'fp': 29, 'fn': 47, 'auroc': 0.8816392479435958, 'auprc': 0.8700721752818596, 'eval_loss': 0.4868150771932399}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 1\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epoch 3 of 20:  10%|â–ˆ         | 2/20 [00:39<05:57, 19.86s/it]INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5754223461805095, 'tp': 156, 'tn': 134, 'fp': 50, 'fn': 29, 'auroc': 0.891539365452409, 'auprc': 0.891354521245363, 'eval_loss': 0.7757770140120324}\n",
      "INFO:simpletransformers.classification.classification_model: No improvement in eval_loss\n",
      "INFO:simpletransformers.classification.classification_model: Current step: 2\n",
      "INFO:simpletransformers.classification.classification_model: Early stopping patience: 2\n",
      "Epochs 2/20. Running Loss:    0.0009: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:15<00:00, 11.89it/s]\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5754223461805095, 'tp': 156, 'tn': 134, 'fp': 50, 'fn': 29, 'auroc': 0.891539365452409, 'auprc': 0.891354521245363, 'eval_loss': 0.7757770140120324}\n",
      "INFO:simpletransformers.classification.classification_model: Patience of 2 steps reached\n",
      "INFO:simpletransformers.classification.classification_model: Training terminated.\n",
      "Epoch 3 of 20:  10%|â–ˆ         | 2/20 [00:56<08:31, 28.43s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to outputs/.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(555,\n",
       " defaultdict(list,\n",
       "             {'global_step': [185, 185, 370, 370, 555, 555],\n",
       "              'train_loss': [0.313232421875,\n",
       "               0.313232421875,\n",
       "               0.056640625,\n",
       "               0.056640625,\n",
       "               0.00090789794921875,\n",
       "               0.00090789794921875],\n",
       "              'mcc': [0.32644187856724644,\n",
       "               0.32644187856724644,\n",
       "               0.590999620240559,\n",
       "               0.590999620240559,\n",
       "               0.5754223461805095,\n",
       "               0.5754223461805095],\n",
       "              'tp': [51, 51, 138, 138, 156, 156],\n",
       "              'tn': [177, 177, 155, 155, 134, 134],\n",
       "              'fp': [7, 7, 29, 29, 50, 50],\n",
       "              'fn': [134, 134, 47, 47, 29, 29],\n",
       "              'auroc': [0.8220329024676851,\n",
       "               0.8220329024676851,\n",
       "               0.8816392479435958,\n",
       "               0.8816392479435958,\n",
       "               0.891539365452409,\n",
       "               0.891539365452409],\n",
       "              'auprc': [0.813226434485485,\n",
       "               0.813226434485485,\n",
       "               0.8700721752818596,\n",
       "               0.8700721752818596,\n",
       "               0.891354521245363,\n",
       "               0.891354521245363],\n",
       "              'eval_loss': [0.7783598798386594,\n",
       "               0.7783598798386594,\n",
       "               0.4868150771932399,\n",
       "               0.4868150771932399,\n",
       "               0.7757770140120324,\n",
       "               0.7757770140120324]}))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train_model(train_df, eval_df=eval_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_2\n",
      "Running Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 78.44it/s]\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5754223461805095, 'tp': 156, 'tn': 134, 'fp': 50, 'fn': 29, 'auroc': 0.891539365452409, 'auprc': 0.891354521245363, 'eval_loss': 0.7757770140120324}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mcc': 0.5754223461805095, 'tp': 156, 'tn': 134, 'fp': 50, 'fn': 29, 'auroc': 0.891539365452409, 'auprc': 0.891354521245363, 'eval_loss': 0.7757770140120324}\n",
      "[{'guid': 6, 'text_a': \"Vi invito anche a leggere la storia reale della missione del dell'ammiraglio byrd\", 'text_b': None, 'label': 0}, {'guid': 7, 'text_a': \"E un vaccino facoltativo solo per viaggiare ci sta, tant'Ã¨ che per andare in Africa Ã¨ sempre stato obbligatorio il vaccino contro la febbre gialla\", 'text_b': None, 'label': 0}, {'guid': 15, 'text_a': \"In breve tempo, per accedere a Internet, sarÃ  richiesto un documento d'identitÃ  Digitale.\", 'text_b': None, 'label': 1}, {'guid': 18, 'text_a': 'Bisogna proprio dirlo: questi NO-VAX seminano odio ovunque e sono dei teppisti.\\n[ðŸ‡®ðŸ‡¹', 'text_b': None, 'label': 0}, {'guid': 31, 'text_a': \"Israele sembra essere entrata in una crisi politica permanente. Il governo Bennett dopo appena un anno di vita Ã¨ giÃ  caduto, ed Ã¨ caduto su una delle questioni vitali della politica del Paese, quella che riguarda la Cisgiordania. La Cisgiordania Ã¨ il territorio nel quale vivono molti palestinesi e Israele la occupa dal 1967. Su questo territorio, esistono due leggi in omaggio ai principi dell'apartheid sionista. Una che prevede un trat di favore per i coloni israeliani e un'altra che prevede la legge marziale per i nativi palestinesi. Il Parlamento avrebbe dovuto rinnovare questa legge che discrimina il popolo palestinese a luglio, ma il Parlamento israeliano, la Knesset, sembra essere diviso sulla questione. \\n\\nC'Ã¨ chi come Bennett vorrebbe definitivamente annettere la Cisgiordania a Israele, ma questo significherebbe ritrovarsi in minoranza perchÃ© i palestinesi hanno una crescita demografica maggiore degli israeliani. C'Ã¨ chi invece vorrebbe lasciare la Cisgiordania ai palestinesi ma i falchi come Bennett e Netanyahu sono contrari. Israele quindi sta vivendo una contraddizione che mette a rischio la sua stessa esistenza come entitÃ  statuale. Una contraddizione nata per volontÃ  della famiglia Rothschild che ordinÃ² alla Gran Bretagna nel 1917 di procedere alla creazione di uno stato non voluto dagli ebrei ma piuttosto dalla lobby sionista. Sembra che siamo giunti all'implosione di questa insanabile contraddizione. Lo stato eletto dal Nuovo Ordine Mondiale rischia di non sopravvivere a questa crisi che riguarda le radici della sua stessa creazione. \\n\", 'text_b': None, 'label': 1}, {'guid': 32, 'text_a': '11 settembre: la madre di tutti i complottismi\\n\\n', 'text_b': None, 'label': 0}, {'guid': 34, 'text_a': \"Grandissimo successo delle iniziative NO GP in tutte le piazze d'italia: aperitivi vino e pasticcini ovunque.\\nQuesta Ã¨ ForlÃ¬ sabato pomeriggio-sera\\n\\n[ðŸ‡®ðŸ‡¹]( \", 'text_b': None, 'label': 1}, {'guid': 35, 'text_a': \"28 Ottobre 2021\\nATENE, porto del Piero, il principale porto greco: i portuali scioperano, sfondano i cancelli e bloccano il porto.\\n\\nAggiornamento delle ore 20:22: non per vax o pass, ma perchÃ¨ la societÃ  cinese Cosco oltre ad aver comperato quota del 67% del porto del Pireo, impone ai lavoratori ritmi elevati e scarsa sicurezza! chiedo scusa dell'inziale imprecisione.\\n\\nðŸ‡®ðŸ‡¹ \", 'text_b': None, 'label': 0}, {'guid': 44, 'text_a': '2017-08-28\\nIncontro a Bellaria il 7-9-2017 ore 21 \"Quale tutela alla luce delle nuove disposizioni di legge\".\\nRispetto all\\'incontro precedente sulle problematiche legali cui i genitori di bimbi 0-6 anni posso incorrere, (di cui il video qui sopra postato il gg 23 cm), in questo caso verranno trattate quelle dei genitori di ragazzi 6-16 anni relativamente alle vaccinazioni obbligatorie.\\nvolantino a seguire.', 'text_b': None, 'label': 0}, {'guid': 45, 'text_a': \"Ma c'Ã¨ qualcuno qui che non si fida della scienza e degli scienziati?\\n[ðŸ‡®ðŸ‡¹ \", 'text_b': None, 'label': 0}, {'guid': 50, 'text_a': \"E mi pregio di concludere i post di questa sera con un terzo intervento, anzi, appello rivolto da giorgio Agamben, un filosofo, alla sensibilitÃ  personale ed umana dei senatori, perchÃ¨ riescano ad inquadrare, al di fuori ed oltre ai luoghi comuni, l'essenza delle scelte che andranno a compiere.\\n\\nðŸ‡®ðŸ‡¹ \", 'text_b': None, 'label': 0}, {'guid': 55, 'text_a': \"20/11/2021 il Mattino web\\nÂ«Covid, pericolo concreto di nuove restrizioni e chi Ã¨ vaccinato non ha l'immunitÃ Â»\\n(no spe...restrizioni per tutti e chi Ã¨ vaccinato non Ã¨ salvo?)\", 'text_b': None, 'label': 0}, {'guid': 56, 'text_a': 'Biomedicines â€¢ 28 GIUGNO 2022](\\n\\nðŸ“ƒ \"__Vaccine mRNA Can Be Detected in Blood at 15 Days Post-Vaccination__\"\\nðŸ‡®ðŸ‡¹ \"**__L\\'mRNA DEL VACCINO PUÃ’ ESSERE RILEVATO NEL SANGUE 15 GIORNI DOPO LA VACCINAZIONE**__\"\\n\\nðŸ“Š \"Tra gennaio e maggio 2021 **abbiamo raccolto campioni di sangue da 16 persone** che avevano recen ricevuto la prima o la seconda dose del vaccino BNT162b2 (Pfizer-BioNTech) [...] **con un\\'etÃ  media di 31 anni. Tutti i partecipanti erano sani**, senza comorbiditÃ  note e non dichiaravano alcuna infezione pregressa da SARS-CoV-2. Nessuno dei partecipanti ha riportato effetti avversi significativi dopo la vaccinazione. Il sangue Ã¨ stato raccolto solo una volta per la maggior parte dei volontari (10 persone) e due o tre volte per gli altri (6 persone), a vari intervalli, da 1 a 27 giorni dopo la vaccinazione [...]. Inoltre, a dicembre 2021, abbiamo arruolato una volontaria sana di 49 anni, dalla quale abbiamo raccolto il sangue 7 volte, a intervalli regolari, a partire dal giorno della terza dose e terminando al giorno 14.\"\\n\\nðŸ“Š \"Abbiamo usato la qPCR [[PCR Quantitativa]( per studiare la presenza e la persistenza dell\\' mRNA sintetico nel sangue dopo una, due e tre dosi di BNT162b2 (Pfizer-BioNTech), supponendo che la biodistribuzione del vaccino sia simile dopo ogni dose.\"\\n\\nâš ï¸ \"La [biodistribuzione]( e la [farmacocinetica]( delle particelle di LNP-mRNA dell\\'attuale generazione di vaccini ad mRNA per il COVID-19 sono di grande interesse, poichÃ© influiscono diret sull\\'[immunogenicitÃ ]( e sulla potenziale tossicitÃ  di queste piattaforme.**\"\\n\\nâ€¼ï¸ \"[...] abbiamo dimostrato che **l\\'mRNA del vaccino** BNT162b2 (Pfizer-BioNTech) **rimane nella circolazione sistemica delle persone vaccinate per almeno 2 settimane, durante le quali probabilmente conserva la sua capacitÃ  di indurre l\\'espressione della proteina Spike nelle cellule e nei tessuti sensibili**.\"\\n\\nðŸ“š **', 'text_b': None, 'label': 1}, {'guid': 60, 'text_a': 'Quante volte abbiamo visto la luna.. nel cielo.. in pieno giorno..?  Io tante volte..', 'text_b': None, 'label': 1}, {'guid': 64, 'text_a': '**REGNO UNITO: DIETROFRONT SUL VACCINO A DONNE INCINTE: \"Ãˆ SCONSIGLIATO\"\\n\\n**Pubblicate sul sito governativo del **Regno Unito** le nuove linee guida riguardo alla somministrazione del vaccino di **Pfizer**. Dopo averlo \"for raccomandato\" alle **donne incinte**, le autoritÃ  fanno ora dietrofront: \"Ã¨ sconsigliato\". \\n\\nâž¡ï¸**__ **__**__', 'text_b': None, 'label': 0}, {'guid': 67, 'text_a': \"Verso le 06:00 ora di Mosca, le truppe ucraine sono sbarcate sulla costa del bacino di Kakhovka, tre chilometri a nord-est della centrale nucleare di Zaporozhye, con due gruppi di sabotaggio composti da un massimo di 60 persone su sette imbarcazioni e hanno tentato di impossessarsi della centrale - Ministero della Difesa russo\\nLa provocazione del regime di Kiev con lo sbarco Ã¨ finalizzata a disturbare l'arrivo del gruppo di lavoro dell'AIEA alla centrale nucleare di Zaporozhye, ha dichiarato il Ministero della Difesa russo. Il dipar ha aggiunto che 4 proiettili ucraini sono esplosi durante il bombardamento a una distanza di 400 metri dalla prima unitÃ  di potenza della ZNPP.\\n\\n\", 'text_b': None, 'label': 1}, {'guid': 74, 'text_a': 'Deve spurgare. Come si usa fare con le lumache prima di essere mangiate...... Questa si chiama anche fame di vendetta', 'text_b': None, 'label': 0}, {'guid': 78, 'text_a': 'Come Hitler ha manipolato 60 milioni di normalissime persone in pochi anni? Rincoglionito\\n\\nMetti in funzione quel cervellino bruciato dalla droga', 'text_b': None, 'label': 0}, {'guid': 79, 'text_a': 'sentinel 5p fa immagini meteo e quindi lasciamo stare', 'text_b': None, 'label': 1}, {'guid': 80, 'text_a': 'PerchÃ© Ã¨ femmina allora nn va bene ? A quanto pare i trogloditi non si sono ancora estinti!!', 'text_b': None, 'label': 0}, {'guid': 81, 'text_a': 'I nodi vengono al pettine anche per la **\"Legione straniera\"** che combatte per il regime di Kiev.\\n \\n**Molti dei \"volontari\" denunciano che i comandanti dell\\'unitÃ  sono implicati in saccheggi, molestie sessuali, aggressioni e invio di soldati impreparati in missioni suicide.\\n**\\nNon si tratta di voci raccolte da blogger filo-russi, bensÃ¬ di un\\'[inchiesta del Kyiv Independent.]( \\n\\nSi punta l\\'indice in particolare su uno dei comandanti - il polacco **Piotr KapuÅ›ciÅ„ski** - membro di un\\'organizzazione criminale in patria, dove Ã¨ ricercato per frode. \\n\\nI \"volontari\" stranieri lo hanno accusato di gravi abusi di potere, per aver ordinato di saccheggiare negozi, minacciato i subalterni con una pistola e molestato sessualmente le dottoresse e le infermiere aggregate alla Legione. \\n\\nSecondo il Kyiv Independent, un rapporto su tali crimini Ã¨ stato presentato al parlamento ucraino e testimonianze scritte sono state inviate anche a Zelensky.', 'text_b': None, 'label': 1}, {'guid': 89, 'text_a': \"Dagli USA Mazzoni, come sempre, in 3 minuti riesce ad offrre una visione d'insieme della salute sia economica che fisica della popolazione, ed inquadra tutto in un contesto internazionale di forze e debolezze\\n[ðŸ‡®ðŸ‡¹\", 'text_b': None, 'label': 0}, {'guid': 92, 'text_a': 'Podcast â€“ Mettere in lockdown i non vaccinati forse non basta\\n\\n', 'text_b': None, 'label': 0}, {'guid': 93, 'text_a': 'Essere baby Sitter, studente, dottore, scienziato,tronista non cambia le idee, le cambiano i fatti\\nNon per nulla settimana scorsa il ministro Speranza, ha respinto i vaccinati colpiti da effetti collaterali allontanandoli e lasciandoli senza risposte\\n\\nPerÃ² Ã¨ piÃ¹ bello farsi bucare che aumenta problemi e non risolve gli attuali', 'text_b': None, 'label': 0}, {'guid': 104, 'text_a': 'Da Medico in 1 anno Ã¨ passato dal \"curare la gente\" a fare l\\'opinionista, poi cantante ora scrittore... Stai a vedere che l\\'anno prossimo si reinventa pittore ðŸ˜‚ðŸ˜‚\\nChe si fa per campare...', 'text_b': None, 'label': 0}, {'guid': 109, 'text_a': \"L'ex presidente dell'Ucraina Poroshenko ha affermato che un tempo aveva accettato gli accordi di Minsk per prendersi la pausa necessaria per preparare l'esercito ucraino a una guerra futura. La confessione Ã¨ utile.\\n\\nIn primo luogo, conferma l'esistenza di una strategia a lungo termine per la militarizzazione dell'Ucraina. Lo sapevano giÃ  tutti. Forse alcuni degli idealisti occidentali credevano che il presidente dell'Ucraina stesse andando agli accordi di Minsk per amore della pace. Fin dall'inizio, da parte russa si era capito che Poroshenko avrebbe sfruttato la pausa, i cui garanti erano Germania e Francia. Diplomatici ed esperti tedeschi e francesi sapevano che la parte ucraina li stava prendendo per il naso. Ma hanno preferito ignorarlo. Ora, dopo aver pubblicamente riconosciuto l'ovvio, Poroshenko ha reso evidente a tutti che i team della Merkel e dell'Olanda erano o sciocchi o bugiardi.  A voi la scelta.\\n\\nIn secondo luogo, Poroshenko ha fornito un'altra prova delle parole di Putin secondo cui l'Ucraina avrebbe attaccato il Donbas per tutti questi anni. Politici e funzionari ucraini di vario livello hanno ripetu confermato questo obiettivo. Ma questa Ã¨ la prima volta che lo fa francamente l'ex capo di Stato. Ora sarÃ  difficile confutare l'esistenza di una strategia di aggressione e di piani concreti per attaccare il Donbass e la Crimea.\\n\\nIn terzo luogo, Poroshenko fornisce argomenti a favore del fatto che, dopo il comple dell'operazione militare, i negoziati con l'Ucraina possono essere condotti solo dal punto di vista della necessitÃ  della sua smilitarizzazione e della assicurazione di garanzie politiche che questa smilitarizzazione non sarÃ  violata. Non si accettano dichiarazioni a vanvera sullo status neutrale dell'Ucraina.\", 'text_b': None, 'label': 1}, {'guid': 120, 'text_a': 'Sono quasi tutti se non tutti cavalli di Troia M5S docet possiamo fidarci solo di Dio e noi stessi...', 'text_b': None, 'label': 1}, {'guid': 121, 'text_a': 'Comitato tecnico scientifico verso lo SCOGLIMENTO ðŸ˜­ðŸ˜­\\n\\n', 'text_b': None, 'label': 0}, {'guid': 129, 'text_a': \"**I piani di viaggio e trasferimento all'estero sono tornati sulla buona strada con l'avvicinarsi dell'estate in molti paesi.  Tuttavia, le restrizioni sanitarie sembrano essere un grosso peso nell'equilibrio per molti.  Mentre alcuni hanno ritardato i loro piani di viaggio fino a giorni migliori, altri semplicemente si sono arresi.  PerchÃ© dovrebbero trasferirsi in un paese con cosÃ¬ tante restrizioni quando il loro paese d'origine ha revocato le restrizioni?\\n\\ni non vaccinati si stanno rivolgendo a paesi comple aperti, come Giamaica, Croazia (aperta ai membri dell'UE non vaccinati), Messico, Argentina, Gabon, Mongolia, ecc**.\\n\\n\", 'text_b': None, 'label': 0}, {'guid': 132, 'text_a': '**L\\'Europarlamentare della Lega, Annalisa Tardino, sul Certificato verde sanitario riesce a pronunciare queste frasi senza ridere**.... ðŸ™„ðŸ˜¡\\n\"Non Ã¨ un Passaporto, non Ã¨ obbligatorio utilizzarlo, non limita gli spos di nessuno, anzi li agevola\".\\npoi aggiunge:\\n\"Per consentire di rilanciare il turismo senza le incertezze vissute oggi da chi viaggia\".\\n\"Nulla di nuovo o di anormale\".\\n\\nðŸ‡®ðŸ‡¹ ', 'text_b': None, 'label': 0}, {'guid': 135, 'text_a': 'Prof. Stefano Scoglio: vi spiego perchÃ© il covid non esiste', 'text_b': None, 'label': 1}, {'guid': 137, 'text_a': \"L'OMS convoca una riunione di emergenza mentre il virus MonkeyPox diventa globale.                                                   \", 'text_b': None, 'label': 0}, {'guid': 155, 'text_a': 'Conclusione la luna ha il diametro di 1.6km e ruota ad una quota di 200km', 'text_b': None, 'label': 0}, {'guid': 156, 'text_a': 'ã€ŠDi fake news ce ne sono a bizzeffe, dappertutto e soprattutto in tempo di guerra: dall\\'altra parte ne raccontano da impuniti. Ma mai mi sognerei di chiedere al governo, al Copasir, ai servizi o alla maggioranza parlamentare di proclamarsi ministero della veritÃ  e stabilire quale sia la veritÃ  e quali le fake news. Ma diamo i numeri?\\nAdesso i servizi segreti ci dicono quale sia la veritÃ ? Con la tradizione che hanno in Italia, fra l\\'altro? Ma a prescindere: siamo matti?\\nE se ti mettono nella lista dei putiniani come fai a difenderti se non c\\'Ã¨ un\\'accusa, non c\\'Ã¨ un reato? Rispondi: \"non Ã¨ vero!\"?\\n**Qui non ci sono reati o compor indegni ma sotto accusa sono le idee di queste persone!** Tra l\\'altro non le conosco e potrei non essere d\\'accordo con quello che dicono ma non mi interessa, non Ã¨ questo il punto.\\nQuesta roba non puÃ² esistere! E non puÃ² esistere che nessuno vada in parlamento a chiedere chi abbia commissionato questa roba e che i giornali non si scandalizzino.ã€‹\\n\\nMarco Travaglio, 7 giugno 2022.', 'text_b': None, 'label': 0}, {'guid': 167, 'text_a': 'Due anni Delta piuttosto rilevante.\\nIl 21 ci sarÃ  la decisione della FED sui tassi dâ€™interesse, 23/24/25 date da seguire con attenzione per via delle tante coincidenze descritte nelle ultime due settimane. WWG1WGA', 'text_b': None, 'label': 1}, {'guid': 188, 'text_a': '**UN\\'ALTRA MANINA HA RITOCCATO LA PAGINA DEL NHS DEDICATA AL VAIOLO DELLE SCIMMIE **\\n\\nFino al [19 maggio 2022]( (vedi immagini allegate), [nella pagina del NHS]( (Sistema sanitario nazionale del Regno Unito) dedicata al vaiolo delle scimmie (Monkeypox), era presente la seguente frase che Ã¨ stata comple rimossa:\\n\\n\"__It\\'s very uncommon to get monkeypox from a person with the infection because it does not spread easily between people__.\"\\n\\nCioÃ¨:\\n\"__Ãˆ molto raro contrarre il vaiolo delle scimmie da una persona con l\\'infezione perchÃ© non si diffonde facilmente tra le persone__.\"\\n\\nðŸ¤”\\n\\nðŸŸ¥ ', 'text_b': None, 'label': 1}, {'guid': 193, 'text_a': '__**\"IL DOVERE DI ISTRUIRSI\" __** \\nPerÃ² Pregliasco questo non sembra averlo capito ancora bene: dovrebbe  \"ripassarlo\" prima di esprimere pareri pubblici... come ha fatto qui: ', 'text_b': None, 'label': 0}, {'guid': 194, 'text_a': 'âš ï¸âš¡ï¸Confine messicano-AMERICANO: TUNNEL narcotrafficanti SCOPERTI a Tijuana\\n17/05/2022 (lapresse.ca)\\n(Los Angeles) Un tunnel lungo 530 metri, che collega la cittÃ  di San Diego in California americana alla cittÃ  messicana di Tijuana, Ã¨ stato scoperto dalla polizia statunitense dopo un\\'indagine traffico di droga, ha annunciato American Justice Lunedi.\\nIl procuratore federale Randy Grossman dice:\\n\\n \"Provvederemo a rimuovere tutti i passaggi di contrabbando sotterranei trovati, per evitare che  droghe illecite e qualsiasi tipo di Traffico, di raggiungere le nostre strade e distruggere le nostre famiglie\" ðŸ’¥\\n\\nC\\'Ã¨ un  tunnel artigianale largo solo 1,2 metri, un tunnel che si puÃ² costruire a 2 metri all\\'ora ðŸ˜³\\nUn tunnel Ã¨ favorevole al traffico di droga, traffico di armi e quindi pedocriminalitÃ \\n\\n \\n\\n\\n   ðŸ‘ˆ', 'text_b': None, 'label': 0}, {'guid': 200, 'text_a': '29/10/21]( [Case report]\\n\\nðŸ“„ __\"Anaphylaxis after Moderna COVID-19 vaccine\"\\nðŸ‡®ðŸ‡¹ \"Anafilassi dopo il vaccino Moderna COVID-19\"__\\n\\nðŸ“Š \"L\\'[ACAAI]( COVID-19 Task Force__ raccomanda [...] chiunque riceva il vaccino dovrebbe essere sottoposto a controlli per il possibile rischio di reazione allergica al vaccino mRNA COVID-19.\"\\nâš ï¸ \"**se un paziente ha una grave reazione allergica entro 4 ore dalla prima dose di Moderna o Pfizer-BioNTech, non dovrebbe ricevere la seconda dose**. [...] La maggior parte delle reazioni anafilattiche si verifica entro i primi 15 minuti dall\\'iniezione. **I vaccini Moderna o Pfizer-BioNTech non devono essere somministrati a soggetti che hanno una storia nota di reazione allergica a qualsiasi componente del vaccino.** La componente specifica del vaccino non Ã¨ stata ancora identificata; tuttavia il [PEG]( Ã¨ uno degli ingredienti dei vaccini mRNA noti per causare anafilassi. [...] **coloro che ne sono allergici avranno una reazione grave o addirittura fatale**.\"', 'text_b': None, 'label': 1}, {'guid': 204, 'text_a': 'Che qualcuno prenda a te per il collo non sarebbe meglio?', 'text_b': None, 'label': 0}, {'guid': 210, 'text_a': \"ma come ho detto solo un sole a parabola.. puo' giustificare la teoria del modello sferico\", 'text_b': None, 'label': 0}, {'guid': 215, 'text_a': 'Alla fine piu osservo un albero o una pianta ... e capisco che le loro foglie sono dei pannelli solari...', 'text_b': None, 'label': 0}, {'guid': 228, 'text_a': 'Credo sia difficile, non Ã¨ stupido il tipo. Al max lo trovi su sileri', 'text_b': None, 'label': 0}, {'guid': 234, 'text_a': 'Che figura di ðŸ’©...\\nAttacco hacker a Regione Lazio, con intrusione cryptolocker.....\\nE chiamano l\\'FBI per cercare dei marocchini o dei senegalesi che fanno truffe informatiche, spacciandole per hacker no vax!\\nD\\'Amato  \"tutti i dati dell\\'anagrafe vaccinale sono in nostro possesso\"....\\nSicuramente Ã¨ vero, ma tutti cryptati!\\nðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚\\nBasta pagare e risolvono tutto!\\n\\nðŸ‡®ðŸ‡¹ ', 'text_b': None, 'label': 0}, {'guid': 236, 'text_a': 'Biden ha dimenticato in quale paese si trova.\\n\\n\"Sono venuto in Arabia Saudita per aiutare a diffondere i nostri interessi\", ha detto parlando in Israele. ðŸ˜‚', 'text_b': None, 'label': 0}, {'guid': 237, 'text_a': 'Da molto tempo si parla di \"geni\" che predispongono a certe patologie.\\nAlcuni studiosi si stanno impegnando per trovare quali siano quelli cui attribuire la maggior facilitÃ  di contrarre e di sviluppare in maniera piÃ¹  grave la malattia. [Uno studio]( milanese si avvia su questa strada.\\n[ðŸ‡®ðŸ‡¹ ', 'text_b': None, 'label': 0}, {'guid': 246, 'text_a': '\\u200bDavvero Â«200 milaÂ» studenti hanno abbandonato la scuola per la Dad?\\n\\n', 'text_b': None, 'label': 0}, {'guid': 249, 'text_a': \"ðŸ‡¬ðŸ‡§Regno Unito 28.04.22ðŸ‡¬ðŸ‡§\\n\\nðŸ‡®ðŸ‡±ISRAELE: AUMENTO DEL 25% DI EVENTI CARDIOVASCOLARI NEGLI UNDER-40 IN SEGUITO ALLA CAMPAGNA VACCINALE\\n\\nAumento degli eventi cardiovascolari di emergenza tra la popolazione sotto i 40 anni in Israele durante l'introduzione del vaccino e la terza ondata di COVID-19.\\nA dimostrarlo Ã¨ uno studio pubblicato lo scorso 28 aprile 2022 sulla nota rivista scientifica britannica Nature che sottolinea la relazione giÃ  stabilita tra vaccini e miocardite.\\n\\nðŸ‘‰Fonte: nature.com/articles/s41598-022-10928-z\", 'text_b': None, 'label': 1}, {'guid': 252, 'text_a': '21/08/21]( [PRE-PRINT]\\n\\nðŸ“„__ \"Shedding of Infectious SARS-CoV-2 Despite Vaccination\"\\nðŸ‡®ðŸ‡¹ \"Diffusione del SARS-CoV-2 nonostante la vaccinazione\"__\\n\\nðŸ“Š \"La variante Delta del SARS-CoV-2 puÃ² causare elevate cariche virali, Ã¨ al trasmissibile e contiene mutazioni che conferiscono una fuga immunitaria parziale. [...] le indagini sull\\'epidemia suggeriscono che le persone vaccinate possono diffondere [la variante] Delta\"\\n\\nâš ï¸ \"[...] questi risultati indicano che anche le **persone asintomatiche e comple vaccinate possono diffondere il virus infettivo**.\"\\n\\nâš ï¸ \"[...] questi dati indicano che gli individui **vaccinati e non vaccinati** infettati dalla variante Delta **possono trasmettere l\\'infezione**.\"\\n\\nðŸ“š **', 'text_b': None, 'label': 1}, {'guid': 254, 'text_a': 'Inizia a smontarsi anche la farsa del 6 Gennaioâ€¦\\n\\n', 'text_b': None, 'label': 1}, {'guid': 256, 'text_a': \"L'oggetto sferico nel video.. l'ho visto anche io a bassa quota sopra una penisola in Slovenia, purtroppo allora non esistevano i telefonini con tecnologia di oggi, per quei anni era gia' tanto scriversi un messaggio in sms .\", 'text_b': None, 'label': 1}, {'guid': 258, 'text_a': \"Questi sono i dati di ieri. In terapia intensiva sono piene al 10% vuol dire che ci sono ancora il 90% dei posti disponibili. Non ci sono gli ospedali in crisi. Nonostante questo Draghi protrae lo stato di emergenza. E' normale ?\\n\\nðŸ‡®ðŸ‡¹ \", 'text_b': None, 'label': 1}, {'guid': 267, 'text_a': \"La Base di aviano e a circa 100km da Trieste , dato che la base ha un arsenale nucleare americano.. non so se avro' tante speranze... dovro' costruire un rifugio atomico..\", 'text_b': None, 'label': 0}, {'guid': 270, 'text_a': \"Un testimonial di una certa rilevanza! \\n\\nDichiarando di votare repubblicano l'onda che scatenerÃ  sarÃ  come uno tsunami ðŸŽ¯\", 'text_b': None, 'label': 0}, {'guid': 274, 'text_a': 'ðŸ‡ºðŸ‡²ðŸ‡ºðŸ‡¦ **Roger Waters, fondatore dei Pink Floyd, definisce Joe Biden un criminale di guerra **\\n\\nIl musicista, **in un\\'intervista per la CNN politics**, ha affermato che Biden continua ad accendere il fuoco del conflitto ucraino. â€œQuesto Ã¨ un crimine grave. PerchÃ© gli Stati Uniti non chiedono a Zelensky di avviare negoziati con Mosca per porre fine a questo terribile conflitto?\"\\n\\nRoger Waters ha anche sottolineato che l\\'operazione speciale per proteggere il Donbass Ã¨ stata una reazione al tentativo di espandere la NATO fino ai confini della Russia, anche se l\\'organizzazione ha promesso di non farlo.\\n\\n\\n\\n ðŸŽ¯ ðŸŽ¯ ðŸŽ¯', 'text_b': None, 'label': 1}, {'guid': 278, 'text_a': 'Non tutti gli Ukraini sono contro la Russia.\\n**Questa signora prende il microfono ad un militare che arruola per strada e non si fa mancare le parole.\\n**[ðŸ‡®ðŸ‡¹ ', 'text_b': None, 'label': 1}, {'guid': 281, 'text_a': 'Panzana pazzesca del leghista Siri: le misure restrittive funzionano eccome\\n\\n', 'text_b': None, 'label': 1}, {'guid': 285, 'text_a': 'Lascialo stare il berlusca che pensava solo ad arricchirsi e al proprio tornaconto', 'text_b': None, 'label': 0}, {'guid': 292, 'text_a': 'Sapete , tante volte mi sono chiesto, come mai non abbiamo immagini in tempo reale del globo terrestre??', 'text_b': None, 'label': 0}, {'guid': 293, 'text_a': \"resta appunto il fatto che non e' chiaro cosa e' che scatena il virus ad essere piu aggressivo , indipenden dall'eta' della vittima\", 'text_b': None, 'label': 0}, {'guid': 295, 'text_a': '05/01/21](\\n\\nðŸ“„__ \"Herd Immunity to COVID-19: Alluring and Elusive\"\\nðŸ‡®ðŸ‡¹ \"ImmunitÃ  di gregge per COVID-19: allettante e sfuggente\"__\\n\\nðŸ“Š \"Abbiamo davvero bisogno di un vaccino prima che una popolazione possa raggiungere l\\'immunitÃ  di gregge contro la COVID-19? La risposta prudente Ã¨ sÃ¬, ma dobbiamo essere consapevoli che anche con il vaccino **probabilmente non raggiungeremo mai l\\'immunitÃ  di gregge**.\"\\n\\nðŸ“š ', 'text_b': None, 'label': 0}, {'guid': 303, 'text_a': \"**â—ï¸ðŸ‘ Kazakistan si rifiuta di mandare suoi miliari ad ammazzare il popolo ucraino!\\n\\nE NON RICONOSCE L'INDIPENDENZA DEL DOMBASS OCCUPATO DAI RUSSI.\\n\\nTURCHIA HA BLOCCATO L'ACCESSO ALLE NAVI MILITARI RUSSE NEL MAR NERO.\\n\\nTUTTI CONTRO LO PSICOPATICO CRIMINALE PUTIN !**\", 'text_b': None, 'label': 0}, {'guid': 304, 'text_a': \"ho trovato questa penso la migliore.. bene aveva ragione l'ammiraglio Bird che diceva che L'Antartide e' molto piu vasto di quanto la mappa mostra...\", 'text_b': None, 'label': 1}, {'guid': 305, 'text_a': 'ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ qualcuno di sano di menteesiste ancora allora! Ed Ã¨ stato ancora troppo buono', 'text_b': None, 'label': 0}, {'guid': 309, 'text_a': 'Nel video.. visto e rivisto, non mi ero soffermato a questa affermazione di  Aldrin..  ma ora si', 'text_b': None, 'label': 0}, {'guid': 312, 'text_a': 'I BOTS che stanno eliminando su Twitter producono questo effetto\\n\\n', 'text_b': None, 'label': 0}, {'guid': 317, 'text_a': 'Consiglio questa lettura \"Zelensky salito al potere con un colpo di stato, guerra Ã¨ tra Russia e NATO\", intervista a Luciano Canfora.', 'text_b': None, 'label': 1}, {'guid': 322, 'text_a': 'Penso che bisogna chiedere proprio a Robert MaloneðŸ˜…. Nessun biologo al mondo sa queste cose, solo lui', 'text_b': None, 'label': 0}, {'guid': 329, 'text_a': 'Lo studio Pfizer suggerisce che il vaccino Covid-19 sia responsabile dell\\'enorme aumento di epatite tra i bambini mentre il governo del Regno Unito avvia un\\'indagine urgente\\n\\nL\\'UKHSA ha annunciato di aver rilevato alti tassi di infiammazione del fegato tra i bambini, ma ha escluso i virus comuni che causano l\\'epatite.\\n\\n\"La massima concentrazione media al di fuori del sito di iniezione Ã¨ stata osservata nel fegato, equivalente al 21,5% della dose... sono stati osservati effetti epatici, compreso un ingrossamento del fegato\" - Studio Pfizer             ', 'text_b': None, 'label': 1}, {'guid': 338, 'text_a': '19/06/21]( [PRE-PRINT]\\nðŸ“Ž [Clinical Infectious Diseases', 'text_b': None, 'label': 1}, {'guid': 341, 'text_a': 'Occupazione di spazi pubblici ,come quando eravamo a squola .loro hanno l obbligo di servizio pubblico', 'text_b': None, 'label': 0}, {'guid': 342, 'text_a': '**IL CDC STATUNITENSE HA RIMOSSO SILENZIOSAMENTE DAL PROPRIO SITO WEB L\\'AFFERMAZIONE \"__L\\' MRNA E LA PROTEINA SPIKE NON RESTANO A LUNGO NEL CORPO__\"**\\n\\nâž¡ï¸ __La pagina al 21 luglio 2022__: \\n\\n\\nâž¡ï¸ __La pagina oggi__:\\n\\n\\nðŸ“Ž Vedi anche questa pubblicazione del 28 giugno 2022: \"[__L\\'mRNA DEL VACCINO PUÃ’ ESSERE RILEVATO NEL SANGUE 15 GIORNI DOPO LA VACCINAZIONE](\\n\\nðŸ—ž Ne parla anche __La VeritÃ  â€¢ 19 AGOSTO 2022__\\n\\nðŸ“š **', 'text_b': None, 'label': 1}, {'guid': 349, 'text_a': 'Breve e bellissimo intervento su dove la scuola, con ogni suo piccolo o grande intervento legislativo, sta giÃ  portando e le modifiche culturali che ne deriveranno per i nostri ragazzi.\\nItalo Calvino cosÃ¬ si espresse:\\n**__\"Un paese che distrugge la scuola non lo fa mai solo per soldi, perchÃ© le risorse mancano, o i costi sono eccesivi.\\nUn paese che demolisce lâ€™istruzione Ã¨ giÃ  governato da quelli che dalla diffusione del sapere hanno solo da perdereâ€.\\n**__[ðŸ‡®ðŸ‡¹ ', 'text_b': None, 'label': 0}, {'guid': 352, 'text_a': '**Biden offre vaccini alla Nord Corea: nessuna risposta.\\n**Gli Stati Uniti hanno â€œofferto vacciniâ€ contro il Covid-19 alla Corea del Nord, che ha confermato il primo focolaio di coronavirus dallâ€™inizio della pandemia, ma â€œnon hanno avuto rispostaâ€.\\n**Della serie \"come far diventare simpatico anche Kim Jong-un\"\\n**[ðŸ‡®ðŸ‡¹ ', 'text_b': None, 'label': 0}, {'guid': 357, 'text_a': 'Bello sentire Trump che massacra Adolfino in tempi non sospetti\\n\\nTraduzione automatica \\n\\nPorca puttana... Trump sapeva che Putin si sarebbe trasferito in Ucraina con anni di anticipo... Trump e Putin hanno un\\'alleanza al 100%.\\n\\n Guardiamo indietro a questo scambio selvaggio del 25/09/2019.  Ha molto piÃ¹ senso di ciÃ² di cui Trump stava parlando 2 anni fa in merito alla \"corruzione\" in Ucraina.\\n\\n Presta attenzione al linguaggio del corpo di Zelensky mentre Trump parla di Putin \"che risolve il problema dell\\'Ucraina\", in riferimento allo stato profondo e alla corruzione del DNC in Ucraina.\\n\\n Zelensky sembra aver visto un fantasma.  Se guardi l\\'intera intervista, si dimena per TUTTO il tempo.  Visualizzazione molto scomoda del linguaggio del corpo.  Ãˆ cosÃ¬ ovvio che anche l\\'MSM ha ampiamente riferito di quanto fosse imbarazzante.\\n\\n Ora che sappiamo che Zelensky era impegnato a nascondere il coinvolgimento dello Stato ucraino nei biolabs Deep State, (vedi decreto per bruciare i documenti di Metabiota State), possiamo capire PERCHÃ‰ sembrava pietrificato.\\n\\n Trump stava inviando un messaggio a Zelensky.  Trump stava MINACCENDO Zelensky, quando ha detto che lui e Putin \"dovrebbero stare insieme\".\\n\\n Lui sapeva.\\n\\n -Clandestino\\n\\n', 'text_b': None, 'label': 1}, {'guid': 359, 'text_a': \"Dipende, c'Ã¨ una soglia per i sudditi ed una per i regnanti\", 'text_b': None, 'label': 1}, {'guid': 361, 'text_a': '**CLAMOROSO!\\n**Mentre noi parliamo di green pass, la Norvegia torna alla normalitÃ \\n\\nðŸ‡®ðŸ‡¹ ', 'text_b': None, 'label': 0}, {'guid': 366, 'text_a': \"Questa macchina e' stata incredibile per quei anni...  inoltre hanno fatto il modellino giocattolo... ma era impossibile trovarlo il negozio ..perche' andava a ruba...\", 'text_b': None, 'label': 0}, {'guid': 368, 'text_a': 'Ma perchÃ© scomodarsi per un problema che NON CI RIGUARDA\\n\\nLa mia multa (o quello che Ã¨) Ã¨ MARCITA in posta per un mese per poi tornargli indietro con un #VAFFA spirituale ðŸ˜Ž\\n\\n', 'text_b': None, 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "print(result)\n",
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 76.68it/s]\n"
     ]
    }
   ],
   "source": [
    "full_pred = model.predict(eval_df.text.tolist())\n",
    "pred = full_pred[0]\n",
    "raw_pred = full_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that prints the classification report\n",
    "def report_scores(test_label, test_pred):\n",
    "    print(classification_report(test_label, \n",
    "                            test_pred, \n",
    "                            target_names=['non-conspiratorial', 'conspiratorial']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-conspiratorial       0.82      0.73      0.77       184\n",
      "    conspiratorial       0.76      0.84      0.80       185\n",
      "\n",
      "          accuracy                           0.79       369\n",
      "         macro avg       0.79      0.79      0.79       369\n",
      "      weighted avg       0.79      0.79      0.79       369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_scores(eval_df.labels.tolist(), pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b871520dad97a5289b2adf41ad6a240d97c9b001919f4a48e8f233f7be13d0e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
